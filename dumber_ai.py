#This is Dumber AI !!! Do not Argue. lol
import os
import json
import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import hashlib
import pickle
import shutil # Dosya iÅŸlemleri iÃ§in
import time # Gecikme iÃ§in eklendi
import uuid # uuid modÃ¼lÃ¼ import edildi

import gradio as gr
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from rank_bm25 import BM25Okapi
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

from langchain.text_splitter import RecursiveCharacterTextSplitter, SpacyTextSplitter
from langchain.chains import LLMChain # LLMChain import edildi
from langchain.memory import ConversationBufferWindowMemory
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader
from langchain_ollama import OllamaLLM
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain_core.embeddings import Embeddings

# ParentDocumentRetriever iÃ§in yeni importlar
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore # Basit bir dokÃ¼man deposu

from sentence_transformers import SentenceTransformer, CrossEncoder
from convert_encoding import convert_to_utf8_no_bom

from tqdm import tqdm # Ä°lerleme Ã§ubuÄŸu iÃ§in eklendi

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# NLTK data download
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    logger.warning("NLTK 'punkt' or 'stopwords' not found. Attempting to download...")
    try:
        nltk.download('punkt')
        nltk.download('stopwords')
        logger.info("NLTK 'punkt' and 'stopwords' downloaded successfully.")
    except Exception as e:
        logger.error(f"Failed to download NLTK data: {e}. Some features might be affected.")


# Configuration (cachemain_.py'den alÄ±nan ve birleÅŸtirilen Config)
@dataclass
class RAGConfig:
    # Genel Dizin AyarlarÄ±
    vector_store_dir: str = "vectorstore" # ChromaDB'nin saklanacaÄŸÄ± dizin
    documents_dir: str = "documents" # KullanÄ±cÄ±nÄ±n belgelerinin yÃ¼kleneceÄŸi dizin
    chat_history_file: str = "chat_history.json" # Sohbet geÃ§miÅŸi dosyasÄ±
    cache_dir: str = "cache" # Embedding cache ve diÄŸer Ã¶nbelleklerin saklanacaÄŸÄ± dizin
    processed_files_meta_file: str = os.path.join("cache", "processed_files_meta.json") # Ä°ÅŸlenen dosya meta verilerini tutacak dosya

    # LLM AyarlarÄ±
    llm_model: str = "llama3" # Ollama model adÄ±
    ollama_base_url: str = "http://localhost:11434"
    llm_temperature: float = 0.5 # LLM sÄ±caklÄ±ÄŸÄ±

    # Embedding ve Reranker Modelleri
    embeddings_model_name: str = "nomic-ai/nomic-embed-text-v1" # Embedding modeli
    cross_encoder_model_name: str = "cross-encoder/ms-marco-TinyBERT-L-2-v2" # Cross-encoder reranker modeli

    # Chunking AyarlarÄ± (Small-to-Big iÃ§in)
    parent_chunk_size: int = 1024 # Daha bÃ¼yÃ¼k ana (parent) parÃ§alar
    parent_chunk_overlap: int = 256 # Parent parÃ§alar arasÄ± Ã§akÄ±ÅŸma
    child_chunk_size: int = 64  # VektÃ¶rleÅŸtirmek iÃ§in daha kÃ¼Ã§Ã¼k (child) parÃ§alar
    child_chunk_overlap: int = 16 # Child parÃ§alar arasÄ± Ã§akÄ±ÅŸma

    # Retrieval ve Rerank AyarlarÄ±
    bm25_top_k: int = 5 # BM25 ile alÄ±nacak top-k sonuÃ§ sayÄ±sÄ±
    vector_top_k: int = 5 # VektÃ¶r aramasÄ± ile alÄ±nacak top-k sonuÃ§ sayÄ±sÄ±
    top_k_retrieval: int = 30 # Hibrit arama sonrasÄ± (RRF Ã¶ncesi) alÄ±nacak toplam belge sayÄ±sÄ±
    top_k_rerank: int = 10 # Reranking sonrasÄ± alÄ±nacak nihai belge sayÄ±sÄ±

    # DiÄŸer Ayarlar
    max_memory_length: int = 10 # Sohbet geÃ§miÅŸinde tutulacak konuÅŸma sayÄ±sÄ±
    confidence_threshold: float = 0.7 # GÃ¼ven eÅŸiÄŸi (ÅŸu an kullanÄ±lmÄ±yor ama gelecekte eklenebilir)
    embedding_batch_size: int = 64 # Embedding batch boyutu
    vectorstore_add_batch_size: int = 4000 # ChromaDB'ye tek seferde eklenecek child chunk sayÄ±sÄ± (performans iÃ§in)

    # Takip SorularÄ± AyarlarÄ±
    suggestion_template: str = """
    AÅŸaÄŸÄ±daki sohbet geÃ§miÅŸi ve yanÄ±ta gÃ¶re, kullanÄ±cÄ±ya sorulabilecek 3 adet kÄ±sa, alakalÄ± ve Ã§eÅŸitli takip sorusu oluÅŸtur.
    Sorular, kullanÄ±cÄ±nÄ±n daha fazla bilgi edinmesine yardÄ±mcÄ± olmalÄ± veya konuyu daha derinlemesine keÅŸfetmelidir.
    Her bir soruyu yeni bir satÄ±rda liste olarak formatla (Ã¶rn: - Soru 1).
    AynÄ± veya Ã§ok benzer sorularÄ± tekrarlama.
    EÄŸer baÄŸlamda Ã¶nerilecek bir ÅŸey yoksa boÅŸ liste dÃ¶ndÃ¼r.

    Sohbet GeÃ§miÅŸi:
    {chat_history}

    YanÄ±t: {answer}

    OlasÄ± Takip SorularÄ±:
    """
    suggestion_llm_model: str = "llama3" # Takip sorularÄ± iÃ§in farklÄ± bir model kullanÄ±labilir
    suggestion_temperature: float = 0.5

    rag_template: str = """
    Sen bir yapay zeka asistanÄ±sÄ±n. SaÄŸlanan baÄŸlamÄ± kullanarak sorularÄ± yanÄ±tla.
    YalnÄ±zca saÄŸlanan baÄŸlamdan bilgi kullan. YanÄ±tlarÄ±nÄ±zÄ± kÄ±sa ve Ã¶z tutun.
    BaÄŸlamda cevap yoksa, "ÃœzgÃ¼nÃ¼m, bu konu hakkÄ±nda yeterli bilgiye sahip deÄŸilim. LÃ¼tfen baÅŸka bir soru sorun veya bilgi bankasÄ±nÄ± gÃ¼ncelleyin." deyin.
    Kesinlikle uydurma bilgi verme.
    Soruya baÄŸlÄ± olarak takip sorularÄ± Ã¶nermeyi unutma.

    KonuÅŸma GeÃ§miÅŸi:
    {chat_history}

    BaÄŸlam:
    {context}

    Soru: {question}
    YanÄ±t:
    """

config = RAGConfig()

# CUDA Setup
logger.info("ğŸš€ CUDA Test")
if torch.cuda.is_available():
    logger.info("âœ… CUDA available.")
    logger.info(f"ğŸ§  Device: {torch.cuda.get_device_name(torch.cuda.current_device())}")
    device = "cuda"
else:
    logger.info("âŒ CUDA not available, CPU kullanÄ±lacak.")
    device = "cpu"
torch.cuda.empty_cache()

# Gerekli dizinleri oluÅŸtur
os.makedirs(config.documents_dir, exist_ok=True)
os.makedirs(config.vector_store_dir, exist_ok=True)
os.makedirs(config.cache_dir, exist_ok=True)


# RAG-Fusion (Reciprocal Rank Fusion - RRF) fonksiyonu
def reciprocal_rank_fusion(
    ranked_lists: List[List[Tuple[Document, float]]], k: int = 60
) -> List[Tuple[Document, float]]:
    """
    Reciprocal Rank Fusion (RRF) kullanarak birden fazla sÄ±ralÄ± listeyi birleÅŸtirir.
    Daha dÃ¼ÅŸÃ¼k rank daha iyidir. Ä°lk eleman rank 1.
    """
    fused_scores = {}
    k_rrf = 60.0 # RRF iÃ§in sabit k deÄŸeri (genellikle 60)

    # Her bir sÄ±ralÄ± liste Ã¼zerinde dÃ¶n
    for ranked_list in ranked_lists:
        # Check if ranked_list is empty to avoid errors with enumerate
        if not ranked_list:
            continue
        for rank, (doc, original_score) in enumerate(ranked_list):
            # Benzersiz ID'yi metadatadan al veya iÃ§erikten oluÅŸtur
            doc_id = doc.metadata.get('id')
            if doc_id is None:
                doc_id = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()
                doc.metadata['id'] = doc_id # ID'yi belgeye ekle
            
            if doc_id not in fused_scores:
                fused_scores[doc_id] = {'doc': doc, 'score': 0.0}
            
            # RRF formÃ¼lÃ¼: 1 / (k_rrf + rank)
            fused_scores[doc_id]['score'] += 1.0 / (k_rrf + rank + 1) # Rank 0'dan baÅŸladÄ±ÄŸÄ± iÃ§in +1

    # Fused skorlara gÃ¶re sÄ±rala
    sorted_results = sorted(fused_scores.values(), key=lambda x: x['score'], reverse=True)
    
    # Orjinal Document objelerini ve fÃ¼zyon skorlarÄ±nÄ± dÃ¶ndÃ¼r
    return [(item['doc'], item['score']) for item in sorted_results[:k]]


class AdvancedEmbeddings(Embeddings):
    """GeliÅŸmiÅŸ embedding sÄ±nÄ±fÄ± - cache ve batch processing ile"""

    def __init__(self, model_name: str = config.embeddings_model_name):
        self.model_name = model_name
        self.cache_file = os.path.join(config.cache_dir, "embeddings_cache.pkl")
        self.cache = self._load_cache()

        try:
            self.model = SentenceTransformer(
                model_name,
                device=device,
                trust_remote_code=True
            )
            logger.info(f"âœ… Embedding model loaded on {device}")
        except Exception as e:
            logger.error(f"âŒ Model loading failed: {e}. Fallback to CPU.")
            self.model = SentenceTransformer(
                model_name,
                device="cpu",
                trust_remote_code=True
            )
            logger.info("âœ… Fallback to CPU mode.")

    def _load_cache(self) -> Dict:
        os.makedirs(config.cache_dir, exist_ok=True)
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e:
                logger.warning(f"âŒ Embedding cache could not be loaded from {self.cache_file} ({e}). Starting with empty cache.")
                return {}
        return {}

    def _save_cache(self):
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            logger.error(f"âŒ Failed to save embedding cache: {e}")

    def _get_cache_key(self, text: str) -> str:
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def embed_documents(self, texts: List[str], batch_size: int = None) -> List[List[float]]:
        if batch_size is None:
            batch_size = config.embedding_batch_size

        if not texts: 
            logger.warning("AdvancedEmbeddings.embed_documents received an empty list of texts. Returning empty embeddings.")
            return []

        embeddings = []
        uncached_texts = []
        uncached_indices = []

        for i, text in enumerate(texts):
            cache_key = self._get_cache_key(text) 
            if cache_key in self.cache:
                embeddings.append(self.cache[cache_key])
            else:
                embeddings.append(None)
                uncached_texts.append(text)
                uncached_indices.append(i)

        if uncached_texts:
            logger.info(f"ğŸ”„ Embedding {len(uncached_texts)} new documents (in batches of {batch_size})...")
            new_embeddings_list = []
            
            num_workers = 0 # Default to 0 for Windows compatibility, or os.cpu_count() - 1 for Linux/macOS
            
            for i in tqdm(range(0, len(uncached_texts), batch_size), desc="Embedding Batches"):
                batch = uncached_texts[i:i+batch_size]
                batch_embeddings = self.model.encode(
                    batch,
                    show_progress_bar=False,
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    device=self.model.device,
                    num_workers=num_workers
                )
                new_embeddings_list.extend(batch_embeddings)

            for i, text_idx in enumerate(uncached_indices):
                text = uncached_texts[i]
                embedding_data = new_embeddings_list[i].tolist()
                cache_key = self._get_cache_key(text)
                self.cache[cache_key] = embedding_data
                embeddings[text_idx] = embedding_data

            self._save_cache()
        else:
            logger.info("âœ… All embeddings found in cache, skipping embedding generation.")

        final_embeddings = []
        for emb in embeddings:
            if emb is None:
                logger.error("Hata: Bir embedding boÅŸ kaldÄ±, bu olmamalÄ±ydÄ±. BoÅŸ string embedding'i oluÅŸturuluyor.")
                final_embeddings.append(self.model.encode("", convert_to_numpy=True, normalize_embeddings=True, device=self.model.device).tolist())
            else:
                final_embeddings.append(emb)

        if not final_embeddings and texts: 
             logger.error("Hata: Girdi metinleri boÅŸ olmamasÄ±na raÄŸmen AdvancedEmbeddings boÅŸ embedding listesi dÃ¶ndÃ¼rdÃ¼. Bu, modelin veya girdinin sorunlu olduÄŸunu gÃ¶sterir.")
        
        return final_embeddings

    def embed_query(self, text: str) -> List[float]:
        cache_key = self._get_cache_key(text)
        if cache_key in self.cache:
            return self.cache[cache_key]

        embedding = self.model.encode(text, convert_to_numpy=True, normalize_embeddings=True, device=self.model.device)
        self.cache[cache_key] = embedding.tolist()
        self._save_cache()
        return embedding.tolist()


class SemanticChunker:
    """Semantik olarak anlamlÄ± parÃ§alara bÃ¶len sÄ±nÄ±f"""

    def __init__(self, embeddings: AdvancedEmbeddings, threshold: float = 0.75):
        self.embeddings = embeddings
        self.threshold = threshold

    def split_text(self, text: str, metadata: Dict = None) -> List[Document]:
        sentences = sent_tokenize(text)
        if len(sentences) < 2:
            return [Document(page_content=text, metadata=metadata or {})]

        sentence_embeddings = self.embeddings.model.encode(sentences, convert_to_numpy=True, device=self.embeddings.model.device)

        chunks = []
        current_chunk = [sentences[0]]

        for i in range(1, len(sentences)):
            similarity = cosine_similarity(
                [sentence_embeddings[i-1]],
                [sentence_embeddings[i]]
            )[0][0]

            if similarity > self.threshold:
                current_chunk.append(sentences[i])
            else:
                chunk_text = " ".join(current_chunk)
                chunks.append(Document(
                    page_content=chunk_text,
                    metadata={**(metadata or {}), "chunk_type": "semantic"}
                ))
                current_chunk = [sentences[i]]

        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunks.append(Document(
                page_content=chunk_text,
                metadata={**(metadata or {}), "chunk_type": "semantic"}
            ))

        return chunks


class HybridRetriever:
    """Hibrit arama: Semantic + Keyword-based (BM25) + Reranking + RRF"""

    def __init__(self, parent_retriever: ParentDocumentRetriever, bm25_documents: List[Document], embeddings: AdvancedEmbeddings):
        self.parent_retriever = parent_retriever # ParentDocumentRetriever instance'Ä±
        self.bm25_documents = bm25_documents # BM25 iÃ§in tÃ¼m child dokÃ¼manlarÄ±
        self.embeddings = embeddings # Embedding sÄ±nÄ±fÄ±
        self.reranker = CrossEncoder(config.cross_encoder_model_name, device=device) # Reranker'Ä± da GPU'ya taÅŸÄ±
        
        # Stop words: BaÅŸlangÄ±Ã§ta boÅŸ bir set ile gÃ¼venli baÅŸlatma
        self.stop_words = set()
        try:
            # TÃ¼rkÃ§e ve Ä°ngilizce stop word'leri birleÅŸtirme
            nltk_stop_words = set(stopwords.words('english'))
            nltk_stop_words.update(stopwords.words('turkish'))
            self.stop_words = nltk_stop_words
        except LookupError:
            logger.warning("NLTK stopwords could not be loaded. Running without stopwords.")
        
        # BM25 indeksi oluÅŸtur
        self.bm25 = self._build_bm25_index()

    def _build_bm25_index(self):
        """BM25 indeksi oluÅŸtur"""
        # Sadece BM25 iÃ§in belge iÃ§eriklerini tokenize et
        corpus = []
        for doc in self.bm25_documents:
            # BM25 iÃ§in kullanÄ±lan dokÃ¼man listesi
            tokens = word_tokenize(doc.page_content.lower())
            tokens = [t for t in tokens if t.isalnum() and t not in self.stop_words]
            corpus.append(tokens)
        
        if not corpus:
            logger.warning("BM25 index cannot be built: No valid document content found for tokenization.")
            return None
        return BM25Okapi(corpus)

    def _preprocess_query(self, query: str) -> str:
        """Query'yi zenginleÅŸtir (isteÄŸe baÄŸlÄ±)"""
        expanded_terms = []
        tokens = word_tokenize(query.lower())
        for token in tokens:
            if token.isalnum() and token not in self.stop_words:
                expanded_terms.append(token)
            # if token in ["nasÄ±l", "how"]:
            #     expanded_terms.extend(["yÃ¶ntem", "method", "way"])
            # elif token in ["nedir", "what"]:
            #     expanded_terms.extend(["tanÄ±m", "definition", "aÃ§Ä±klama"])
        return " ".join(expanded_terms)

    def retrieve(self, query: str, k: int = config.top_k_retrieval) -> List[Document]:
        """Hibrit retrieval (RRF ve Reranking ile)"""
        processed_query = self._preprocess_query(query)

        # 1. Semantic search (ParentDocumentRetriever'Ä±n underlying vectorstore'dan child chunks al)
        # ParentDocumentRetriever'Ä±n child retriever'Ä±nÄ± kullanarak skorlu child chunk'larÄ± al
        # semantic_child_results = self.parent_retriever.vectorstore.similarity_search_with_score(
        #     processed_query, k=k*3 # RRF ve reranking iÃ§in daha fazla al
        # )
        # Langchain'in ParentDocumentRetriever'Ä± doÄŸrudan retrieve metoduyla parent belgeleri dÃ¶ndÃ¼rÃ¼r.
        # Bizim burada child belgeleri skorlarÄ±yla almamÄ±z gerekiyor.
        # Bu, Chroma'nÄ±n search metodunu kullanarak yapÄ±labilir.
        semantic_child_results_with_scores = self.parent_retriever.vectorstore.similarity_search_with_score(
            processed_query, k=config.vector_top_k * 3 # VektÃ¶r arama top-k'sÄ±nÄ±n 3 katÄ±nÄ± al
        )
        # DokÃ¼man ve skor tuple listesini hazÄ±rla
        semantic_results_for_rrf = [(doc, score) for doc, score in semantic_child_results_with_scores]


        # 2. BM25 search (child dokÃ¼manlar Ã¼zerinde)
        bm25_child_results_with_scores = []
        if self.bm25 is not None:
            query_tokens = word_tokenize(processed_query.lower())
            query_tokens = [t for t in query_tokens if t.isalnum() and t not in self.stop_words]
            
            if query_tokens:
                bm25_scores = self.bm25.get_scores(query_tokens)
                # BM25 sonuÃ§larÄ±nÄ± (document, score) formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
                # Top k kadar olanlarÄ± alalÄ±m ve bir ranking verelim
                ranked_bm25_docs = sorted(zip(self.bm25_documents, bm25_scores), key=lambda x: x[1], reverse=True)[:config.bm25_top_k * 3]
                bm25_child_results_with_scores = [(doc, score) for doc, score in ranked_bm25_docs]

        # 3. RRF ile birleÅŸtirme
        # reciprocal_rank_fusion'a list of lists of (Document, score) vermemiz gerekiyor
        all_ranked_lists = [semantic_results_for_rrf, bm25_child_results_with_scores]
        fused_results = reciprocal_rank_fusion(all_ranked_lists, k=config.top_k_retrieval)
        
        # Sadece Document objelerini al
        fused_documents = [doc for doc, score in fused_results]

        if not fused_documents:
            logger.warning("RRF sonrasÄ± belge bulunamadÄ±. BoÅŸ liste dÃ¶ndÃ¼rÃ¼lÃ¼yor.")
            return []

        # 4. Reranking (Cross-encoder ile)
        # Reranker iÃ§in belge iÃ§eriÄŸi ve sorgu Ã§iftlerini hazÄ±rla
        sentence_pairs = [[query, doc.page_content] for doc in fused_documents]
        
        if not sentence_pairs:
            logger.warning("Reranking iÃ§in sentence_pairs boÅŸ. Reranking atlanÄ±yor.")
            return fused_documents[:config.top_k_rerank] # Sadece RRF sonuÃ§larÄ±nÄ±n baÅŸÄ±nÄ± dÃ¶ndÃ¼r

        # Rerank skorlarÄ±nÄ± hesapla
        # Reranker batch boyutunu yÃ¶netebilir, ancak Ã§ok bÃ¼yÃ¼k deÄŸilse tek seferde gÃ¶nderilebilir.
        rerank_scores = self.reranker.predict(sentence_pairs).tolist()

        # DokÃ¼manlarÄ± rerank skorlarÄ±na gÃ¶re sÄ±rala
        reranked_documents_with_scores = sorted(zip(fused_documents, rerank_scores), key=lambda x: x[1], reverse=True)

        # Sadece en iyi N reranked dokÃ¼manÄ± al
        final_retrieved_documents = [doc for doc, score in reranked_documents_with_scores[:config.top_k_rerank]]
        
        return final_retrieved_documents


class RAGSystem:
    def __init__(self):
        self.vectorstore: Optional[Chroma] = None
        self.bm25_retriever: Optional[BM25Okapi] = None # HybridRetriever iÃ§inde olacak
        self.embeddings = AdvancedEmbeddings(config.embeddings_model_name)
        self.cross_encoder = CrossEncoder(config.cross_encoder_model_name, device=device) # Init sÄ±rasÄ±nda cihaz belirt
        self.llm = OllamaLLM(model=config.llm_model, base_url=config.ollama_base_url, temperature=config.llm_temperature)
        self.conversation_chain = None
        self.chat_history_memory = ConversationBufferWindowMemory(memory_key="chat_history", return_messages=True, k=config.max_memory_length, input_key="question")
        self.documents_for_bm25: List[Document] = [] # BM25 iÃ§in tÃ¼m child belgeleri saklayacak (artÄ±k doÄŸrudan Document objeleri)
        self.id_to_document_map: Dict[str, Document] = {} # Parent belgenin hashinden tam belgeye ulaÅŸÄ±m iÃ§in

        self.vectorstore_path = os.path.join(config.vector_store_dir, "chroma_db")
        self.bm25_index_path = os.path.join(config.cache_dir, "bm25_index.pkl") # cache klasÃ¶rÃ¼ne alÄ±ndÄ±
        self.id_map_path = os.path.join(config.cache_dir, "id_map.pkl") # cache klasÃ¶rÃ¼ne alÄ±ndÄ±
        self.parent_store_path = os.path.join(config.cache_dir, "parent_store.pkl") # cache klasÃ¶rÃ¼ne alÄ±ndÄ±
        self.processed_files_meta_path = config.processed_files_meta_file
        self.processed_files_meta = self._load_processed_files_meta()
        self.parent_document_store = InMemoryStore() # ParentDocumentRetriever iÃ§in store

        self.parent_retriever: Optional[ParentDocumentRetriever] = None
        self.hybrid_retriever: Optional[HybridRetriever] = None

        logger.info("RAGSystem baÅŸlatÄ±lÄ±yor...")
        self.load_knowledge_base()
        self.initialize_llm_chain()
        logger.info("RAGSystem baÅŸarÄ±yla baÅŸlatÄ±ldÄ±.")

    def _load_processed_files_meta(self) -> Dict[str, Dict]:
        """Ä°ÅŸlenmiÅŸ dosyalarÄ±n meta verilerini yÃ¼kler."""
        if os.path.exists(self.processed_files_meta_path):
            try:
                with open(self.processed_files_meta_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"Ä°ÅŸlenmiÅŸ dosyalar meta verisi yÃ¼klenemedi: {e}. BoÅŸ meta veri ile devam ediliyor.")
                return {}
        return {}

    def _save_processed_files_meta(self):
        """Ä°ÅŸlenmiÅŸ dosyalarÄ±n meta verilerini kaydeder."""
        os.makedirs(os.path.dirname(self.processed_files_meta_path), exist_ok=True)
        with open(self.processed_files_meta_path, 'w', encoding='utf-8') as f:
            json.dump(self.processed_files_meta, f, indent=4)

    def _get_file_hash(self, file_path: str) -> str:
        """DosyanÄ±n MD5 hash'ini hesaplar."""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
        except Exception as e:
            logger.error(f"Dosya hash'i hesaplanÄ±rken hata oluÅŸtu {file_path}: {e}")
            return "" # Hata durumunda boÅŸ dÃ¶ndÃ¼r
        return hash_md5.hexdigest()

    def _get_text_splitter(self, chunk_size: int, chunk_overlap: int):
        return RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False,
        )
    
    def _initialize_parent_document_retriever(self):
        """ParentDocumentRetriever'Ä± baÅŸlatÄ±r veya yeniden baÅŸlatÄ±r."""
        # Chroma'nÄ±n baÅŸlatÄ±lmÄ±ÅŸ olmasÄ± gerekiyor
        if self.vectorstore is None:
            logger.error("ParentDocumentRetriever baÅŸlatÄ±lamadÄ±: Chroma vectorstore mevcut deÄŸil.")
            return

        child_splitter = self._get_text_splitter(config.child_chunk_size, config.child_chunk_overlap)
        parent_splitter = self._get_text_splitter(config.parent_chunk_size, config.parent_chunk_overlap)

        self.parent_retriever = ParentDocumentRetriever(
            vectorstore=self.vectorstore,
            docstore=self.parent_document_store,
            child_splitter=child_splitter,
            parent_splitter=parent_splitter,
            search_kwargs={'k': config.vector_top_k} # ParentDocumentRetriever'Ä±n kendi arama k'sÄ±
        )
        logger.info("ParentDocumentRetriever baÅŸlatÄ±ldÄ±.")

    def add_documents_to_knowledge_base(self, documents: List[Document], force_rebuild: bool = False):
        if not documents:
            logger.warning("Eklenecek belge bulunamadÄ±.")
            return

        logger.info(f"{len(documents)} adet belge iÅŸleniyor...")

        if force_rebuild:
            logger.info("Zorla yeniden oluÅŸturma: Mevcut bilgi bankasÄ± temizleniyor.")
            if os.path.exists(self.vectorstore_path):
                shutil.rmtree(self.vectorstore_path)
            if os.path.exists(self.bm25_index_path):
                os.remove(self.bm25_index_path)
            if os.path.exists(self.id_map_path):
                os.remove(self.id_map_path)
            if os.path.exists(self.parent_store_path):
                os.remove(self.parent_store_path)
            if os.path.exists(self.processed_files_meta_path):
                os.remove(self.processed_files_meta_path)
            
            self.vectorstore = None
            self.documents_for_bm25 = []
            self.id_to_document_map = {}
            self.processed_files_meta = {}
            self.parent_document_store = InMemoryStore() # SÄ±fÄ±rla
            logger.info("Mevcut Chroma veritabanÄ±, BM25 indeksi ve meta verileri temizlendi.")
        
        # Chroma'yÄ± (yeniden) baÅŸlat / EÄŸer yoksa oluÅŸtur
        if self.vectorstore is None:
            self.vectorstore = Chroma(
                persist_directory=self.vectorstore_path,
                embedding_function=self.embeddings
            )
            logger.info("Chroma veritabanÄ± baÅŸlatÄ±ldÄ±.")
        
        # ParentDocumentRetriever'Ä± baÅŸlat
        if self.parent_retriever is None or force_rebuild: # Her zaman yeniden baÅŸlat veya ilk kez baÅŸlat
            self._initialize_parent_document_retriever()
        
        new_docs_to_add = []
        for doc in documents:
            file_path = doc.metadata.get("source") # Langchain loader'lar "source" metadata'sÄ± ekler
            if file_path:
                file_hash = self._get_file_hash(file_path)
                last_modified = os.path.getmtime(file_path) # Son deÄŸiÅŸiklik zamanÄ±

                if file_path in self.processed_files_meta and \
                   self.processed_files_meta[file_path]["hash"] == file_hash and \
                   self.processed_files_meta[file_path]["last_modified"] >= last_modified:
                    logger.info(f"'{os.path.basename(file_path)}' zaten iÅŸlenmiÅŸ ve deÄŸiÅŸmemiÅŸ. AtlanÄ±yor.")
                else:
                    new_docs_to_add.append(doc)
                    self.processed_files_meta[file_path] = {
                        "hash": file_hash,
                        "last_modified": last_modified,
                        "processed_at": datetime.now().isoformat()
                    }
            else:
                # Kaynak bilgisi olmayan belgeleri de ekle
                new_docs_to_add.append(doc)
                logger.warning(f"Belge iÃ§in kaynak yolu bulunamadÄ±: {doc.metadata.get('id', 'Bilinmeyen belge')}. DeÄŸiÅŸiklikler takip edilemeyecek.")


        if not new_docs_to_add:
            logger.info("HiÃ§bir yeni veya gÃ¼ncellenmiÅŸ belge bulunamadÄ±. Bilgi bankasÄ± gÃ¼ncellenmedi.")
            return

        logger.info(f"{len(new_docs_to_add)} adet yeni/gÃ¼ncellenmiÅŸ belge bilgi bankasÄ±na ekleniyor...")
        
        # ParentDocumentRetriever'a belgeleri ekle
        self.parent_retriever.add_documents(new_docs_to_add)
        
        # Chroma'yÄ± kalÄ±cÄ± hale getir
        self.vectorstore.persist()
        logger.info("Chroma veritabanÄ± kalÄ±cÄ± hale getirildi.")

        # BM25 iÃ§in child belgeleri gÃ¼ncelle
        # ParentDocumentRetriever'Ä±n child belgelerini almanÄ±n doÄŸrudan bir yolu yok,
        # bu yÃ¼zden child splitter ile belgeleri yeniden parÃ§alamamÄ±z gerekiyor.
        # Veya ParentDocumentRetriever'Ä±n docstore'undan child ID'lerini alÄ±p,
        # bu ID'lerle child belgelerin content'ini toplayabiliriz.
        # Basitlik iÃ§in, ÅŸimdilik tÃ¼m belgeleri tekrar child olarak parÃ§alayalÄ±m.
        
        # Mevcut tÃ¼m parent dokÃ¼manlarÄ± docstore'dan al
        all_parent_ids = list(self.parent_document_store.yield_keys())
        all_parent_docs = self.parent_document_store.mget(all_parent_ids)
        
        self.documents_for_bm25 = [] # BM25 iÃ§in listeyi sÄ±fÄ±rla
        self.id_to_document_map = {} # HaritayÄ± sÄ±fÄ±rla

        child_splitter = self._get_text_splitter(config.child_chunk_size, config.child_chunk_overlap)

        for parent_doc in all_parent_docs:
            if parent_doc: # None olmadÄ±ÄŸÄ±ndan emin ol
                # Parent belgenin MD5 hash'ini ID olarak kullan
                parent_doc_id = hashlib.md5(parent_doc.page_content.encode('utf-8')).hexdigest()
                self.id_to_document_map[parent_doc_id] = parent_doc # Parent belgeyi haritaya ekle

                child_chunks = child_splitter.split_documents([parent_doc])
                for chunk in child_chunks:
                    # Child'a parent ID'si ekle
                    chunk.metadata["parent_doc_id"] = parent_doc_id
                    self.documents_for_bm25.append(chunk) # BM25 iÃ§in Document objesini ekle
        
        # BM25 indeksini gÃ¼ncelle
        if self.documents_for_bm25:
            # BM25 iÃ§in sadece content'leri tokenize et
            tokenized_corpus = [word_tokenize(doc.page_content.lower()) for doc in self.documents_for_bm25]
            self.bm25_retriever = BM25Okapi(tokenized_corpus)
            logger.info(f"BM25 indeksi {len(self.documents_for_bm25)} child belge ile gÃ¼ncellendi.")
        else:
            self.bm25_retriever = None
            logger.warning("BM25 indeksi iÃ§in belge bulunamadÄ±.")
        
        # HybridRetriever'Ä± yeniden baÅŸlat
        self.hybrid_retriever = HybridRetriever(self.parent_retriever, self.documents_for_bm25, self.embeddings)
        logger.info("HybridRetriever yeniden baÅŸlatÄ±ldÄ±.")

        self.save_knowledge_base_state()
        self._save_processed_files_meta() # Ä°ÅŸlenmiÅŸ dosyalar meta verisini kaydet
        logger.info(f"Bilgi bankasÄ±na {len(new_docs_to_add)} yeni/gÃ¼ncellenmiÅŸ belge eklendi.")

    def load_knowledge_base(self):
        """KayÄ±tlÄ± bilgi bankasÄ±nÄ± yÃ¼kler."""
        # Chroma yÃ¼kle
        if os.path.exists(self.vectorstore_path) and len(os.listdir(self.vectorstore_path)) > 0:
            try:
                self.vectorstore = Chroma(
                    persist_directory=self.vectorstore_path,
                    embedding_function=self.embeddings
                )
                logger.info("Chroma veritabanÄ± yÃ¼klendi.")
            except Exception as e:
                logger.error(f"Chroma veritabanÄ± yÃ¼klenirken hata oluÅŸtu: {e}. Yeniden oluÅŸturulmasÄ± gerekebilir.")
                self.vectorstore = None
        else:
            logger.warning("Chroma veritabanÄ± bulunamadÄ± veya boÅŸ. Yeni bir veritabanÄ± oluÅŸturulacak.")
            self.vectorstore = None # None olarak kalsÄ±n, add_documents_to_knowledge_base oluÅŸturacak

        # BM25 indeksi ve ID haritasÄ±nÄ± yÃ¼kle
        if os.path.exists(self.bm25_index_path) and os.path.exists(self.id_map_path) and os.path.exists(self.parent_store_path):
            try:
                with open(self.bm25_index_path, 'rb') as f:
                    self.bm25_retriever = pickle.load(f)
                with open(self.id_map_path, 'rb') as f:
                    self.id_to_document_map = pickle.load(f)
                with open(self.parent_store_path, 'rb') as f:
                    self.parent_document_store = pickle.load(f)
                
                # documents_for_bm25 listesini yeniden oluÅŸtur
                self.documents_for_bm25 = list(self.id_to_document_map.values()) # Veya daha doÄŸru bir ÅŸekilde child dokÃ¼manlarÄ± yÃ¼kle

                logger.info("BM25 indeksi, ID haritasÄ± ve Parent Belge Deposu yÃ¼klendi.")
            except Exception as e:
                logger.error(f"BM25 indeksi, ID haritasÄ± veya Parent Belge Deposu yÃ¼klenirken hata oluÅŸtu: {e}. Yeniden oluÅŸturulmasÄ± gerekebilir.")
                self.bm25_retriever = None
                self.id_to_document_map = {}
                self.parent_document_store = InMemoryStore() # Hata durumunda sÄ±fÄ±rla
                self.documents_for_bm25 = [] # SÄ±fÄ±rla
        else:
            logger.warning("BM25 indeksi, ID haritasÄ± veya Parent Belge Deposu bulunamadÄ±. Yeni oluÅŸturulacak.")
            self.bm25_retriever = None
            self.id_to_document_map = {}
            self.parent_document_store = InMemoryStore() # Yeni oluÅŸtur
            self.documents_for_bm25 = [] # Yeni oluÅŸtur

        # ParentDocumentRetriever ve HybridRetriever'Ä± yÃ¼kleme sonrasÄ± baÅŸlat
        self._initialize_parent_document_retriever()
        # BM25 retriever'Ä± HybridRetriever'a geÃ§irmeden Ã¶nce bm25_documents'Ä± dolduralÄ±m.
        # Burada ParentDocumentRetriever'Ä±n docstore'undan ana belgeleri alÄ±p,
        # bunlarÄ± child splitter ile yeniden parÃ§alayarak bm25_documents'Ä± doldurmalÄ±yÄ±z.
        all_parent_ids = list(self.parent_document_store.yield_keys())
        all_parent_docs = self.parent_document_store.mget(all_parent_ids)
        
        self.documents_for_bm25 = []
        child_splitter = self._get_text_splitter(config.child_chunk_size, config.child_chunk_overlap)
        for parent_doc in all_parent_docs:
            if parent_doc:
                child_chunks = child_splitter.split_documents([parent_doc])
                for chunk in child_chunks:
                    # Child'a parent ID'si ekle (yÃ¼kleme sÄ±rasÄ±nda tekrar ekleyelim)
                    parent_doc_id = hashlib.md5(parent_doc.page_content.encode('utf-8')).hexdigest()
                    chunk.metadata["parent_doc_id"] = parent_doc_id
                    self.documents_for_bm25.append(chunk)

        # BM25 retriever'Ä± yÃ¼kleme sonrasÄ± yeniden oluÅŸtur (HybridRetriever iÃ§inde)
        self.hybrid_retriever = HybridRetriever(self.parent_retriever, self.documents_for_bm25, self.embeddings)
        logger.info("HybridRetriever yÃ¼klendi/baÅŸlatÄ±ldÄ±.")


    def save_knowledge_base_state(self):
        """BM25 indeksi, ID haritasÄ± ve Parent Belge Deposu durumunu kaydeder."""
        os.makedirs(config.cache_dir, exist_ok=True) # Cache dizini mevcut olsun

        # BM25 indeksini kaydet
        if self.bm25_retriever:
            with open(self.bm25_index_path, 'wb') as f:
                pickle.dump(self.bm25_retriever, f)
            logger.info("BM25 indeksi kaydedildi.")
        else:
            logger.warning("BM25 indeksi boÅŸ, kaydedilemedi.")
            if os.path.exists(self.bm25_index_path): # EÄŸer boÅŸsa eski dosyayÄ± sil
                os.remove(self.bm25_index_path)

        # ID haritasÄ±nÄ± kaydet
        with open(self.id_map_path, 'wb') as f:
            pickle.dump(self.id_to_document_map, f)
        logger.info("ID haritasÄ± kaydedildi.")

        # Parent Document Store'u kaydet
        if self.parent_document_store:
            with open(self.parent_store_path, 'wb') as f:
                pickle.dump(self.parent_document_store, f)
            logger.info("Parent Document Store kaydedildi.")
        else:
            logger.warning("Parent Document Store boÅŸ, kaydedilemedi.")
            if os.path.exists(self.parent_store_path):
                os.remove(self.parent_store_path)


    def initialize_llm_chain(self):
        """LLM zincirini baÅŸlatÄ±r."""
        self.llm = OllamaLLM(model=config.llm_model, base_url=config.ollama_base_url, temperature=config.llm_temperature)
        rag_prompt = PromptTemplate(
            template=config.rag_template,
            input_variables=["chat_history", "context", "question"]
        )
        self.conversation_chain = LLMChain(
            llm=self.llm,
            prompt=rag_prompt,
            verbose=False, # Daha az Ã§Ä±ktÄ± iÃ§in False
            memory=self.chat_history_memory,
            callbacks=[],
        )
        logger.info("LLM zinciri yeniden baÅŸlatÄ±ldÄ±.")

    def retrieve_documents(self, query: str) -> List[Document]:
        """Sorguya gÃ¶re ilgili belgeleri getirir (HybridRetriever kullanÄ±larak)."""
        if self.hybrid_retriever is None:
            logger.error("HybridRetriever baÅŸlatÄ±lmamÄ±ÅŸ. Bilgi bankasÄ± yeniden oluÅŸturulmalÄ± veya yÃ¼klenmeli.")
            return []
        
        retrieved_docs = self.hybrid_retriever.retrieve(query)
        logger.info(f"Hybrid Retriever tarafÄ±ndan {len(retrieved_docs)} belge getirildi.")
        return retrieved_docs

    def generate_response(self, query: str, context: str) -> str:
        """LLM kullanarak yanÄ±tÄ± oluÅŸturur."""
        try:
            # LLM zincirinin input key'i "question" olarak ayarlandÄ±ÄŸÄ±ndan,
            # query ve context'i uygun ÅŸekilde iletmeliyiz.
            # KonuÅŸma geÃ§miÅŸi zaten memory tarafÄ±ndan yÃ¶netiliyor.
            response = self.conversation_chain.invoke({
                "context": context,
                "question": query,
                "chat_history": self.chat_history_memory.buffer # chat_history doÄŸrudan prompt'a geÃ§iriliyor
            })
            return response['text']
        except Exception as e:
            logger.error(f"YanÄ±t oluÅŸturulurken hata oluÅŸtu: {e}")
            return "ÃœzgÃ¼nÃ¼m, bir sorun oluÅŸtu. LÃ¼tfen daha sonra tekrar deneyin."

    def generate_suggestions(self, chat_history: str, answer: str) -> List[str]:
        """Takip sorularÄ± Ã¶nerileri oluÅŸturur."""
        try:
            suggestion_llm = OllamaLLM(model=config.suggestion_llm_model, base_url=config.ollama_base_url, temperature=config.suggestion_temperature)
            suggestion_prompt = PromptTemplate(
                template=config.suggestion_template,
                input_variables=["chat_history", "answer"]
            )
            suggestion_chain = LLMChain(llm=suggestion_llm, prompt=suggestion_prompt)
            
            response = suggestion_chain.invoke({
                "chat_history": chat_history,
                "answer": answer
            })
            suggestions_text = response['text'].strip()
            if suggestions_text.lower() == "boÅŸ liste dÃ¶ndÃ¼r": # LLM'in bu Ã§Ä±ktÄ±yÄ± vermesi durumunda
                return []
            return [s.strip() for s in suggestions_text.split('\n') if s.strip()]
        except Exception as e:
            logger.error(f"Takip sorularÄ± oluÅŸturulurken hata oluÅŸtu: {e}")
            return []


# RAG Sistemi Ã¶rneÄŸi
rag_system = RAGSystem()

def get_chat_history_html():
    """Sohbet geÃ§miÅŸini HTML formatÄ±nda dÃ¶ndÃ¼rÃ¼r."""
    history = rag_system.chat_history_memory.buffer
    html_history = ""
    for i, message in enumerate(history):
        if isinstance(message, tuple): # Langchain 0.1.x ve Ã¶nceki sÃ¼rÃ¼mlerde bÃ¶yle olabilir
            speaker = "KullanÄ±cÄ±" if i % 2 == 0 else "Asistan"
            content = message[1]
        else: # Langchain 0.2.x ve sonrasÄ± iÃ§in BaseMessage objeleri
            speaker = "KullanÄ±cÄ±" if message.type == "human" else "Asistan"
            content = message.content
        
        if speaker == "KullanÄ±cÄ±":
            html_history += f"<p style='color: blue;'><b>KullanÄ±cÄ±:</b> {content}</p>"
        else:
            html_history += f"<p style='color: green;'><b>Asistan:</b> {content}</p>"
    return html_history


def respond(message: str, chat_history: List[List[str]]) -> Tuple[str, List[List[str]], List[str]]:
    """Gradio arayÃ¼zÃ¼nden gelen mesaja yanÄ±t verir."""
    logger.info(f"Gelen mesaj: {message}")

    # Sohbet geÃ§miÅŸini gÃ¼ncel (Langchain memory'si zaten yÃ¶netiyor)
    # Gradio'nun chat_history'si sadece UI iÃ§in tutuluyor.
    # rag_system.chat_history_memory.save_context({"question": message}, {"answer": "..."})

    # Belge aramasÄ± yap
    retrieved_docs = rag_system.retrieve_documents(message)
    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
    
    if not context:
        logger.warning("Ä°lgili belge bulunamadÄ±.")
        rag_system.chat_history_memory.save_context({"question": message}, {"answer": "ÃœzgÃ¼nÃ¼m, bu konu hakkÄ±nda yeterli bilgiye sahip deÄŸilim. LÃ¼tfen baÅŸka bir soru sorun veya bilgi bankasÄ±nÄ± gÃ¼ncelleyin."})
        full_response = "ÃœzgÃ¼nÃ¼m, bu konu hakkÄ±nda yeterli bilgiye sahip deÄŸilim. LÃ¼tfen baÅŸka bir soru sorun veya bilgi bankasÄ±nÄ± gÃ¼ncelleyin."
        suggestions = rag_system.generate_suggestions(get_chat_history_html(), full_response)
        return full_response, rag_system.chat_history_memory.buffer, suggestions # Gradio formatÄ±na uygun dÃ¶ndÃ¼r

    logger.info(f"LLM'e gÃ¶nderilen baÄŸlam uzunluÄŸu: {len(context)} karakter.")
    
    # LLM ile yanÄ±t oluÅŸtur
    full_response = rag_system.generate_response(message, context)
    
    # Sohbet geÃ§miÅŸini kaydet (Langchain ConversationBufferWindowMemory zaten yapÄ±yor)
    # Gradio'nun chat_history'sini gÃ¼ncelleyelim.
    # rag_system.chat_history_memory.save_context({"question": message}, {"answer": full_response}) # Zaten LLMChain tarafÄ±ndan yapÄ±lÄ±yor
    
    # Takip sorularÄ± oluÅŸtur
    suggestions = rag_system.generate_suggestions(get_chat_history_html(), full_response)

    # Gradio formatÄ±nda dÃ¶ndÃ¼r
    # Gradio'nun chat_history formatÄ±na uymak iÃ§in
    # rag_system.chat_history_memory.buffer'daki mesajlarÄ± List[List[str]] formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmeliyiz.
    gradio_chat_history = []
    for i in range(0, len(rag_system.chat_history_memory.buffer), 2):
        if i+1 < len(rag_system.chat_history_memory.buffer):
            user_msg = rag_system.chat_history_memory.buffer[i].content
            ai_msg = rag_system.chat_history_memory.buffer[i+1].content
            gradio_chat_history.append([user_msg, ai_msg])

    return full_response, gradio_chat_history, suggestions


def rebuild_knowledge_base_full(progress=gr.Progress()):
    """Bilgi bankasÄ±nÄ± sÄ±fÄ±rdan yeniden oluÅŸturur."""
    progress(0, desc="Bilgi bankasÄ± temizleniyor...")
    logger.info("Bilgi bankasÄ± tam yeniden oluÅŸturma baÅŸlatÄ±ldÄ±.")
    try:
        # data_dir yerine documents_dir kullan
        loader = DirectoryLoader(
            config.documents_dir,
            glob="**/*",
            loader_cls=lambda path: PyPDFLoader(path) if path.endswith('.pdf') else TextLoader(path, encoding='utf-8')
        )
        documents = loader.load()
        logger.info(f"{len(documents)} adet belge yÃ¼klendi.")

        if not documents:
            logger.warning("Yeniden oluÅŸturulacak belge bulunamadÄ±. LÃ¼tfen 'documents' klasÃ¶rÃ¼ne belge ekleyin.")
            return "<p style='color: orange;'>âš ï¸ Yeniden oluÅŸturulacak belge bulunamadÄ±. LÃ¼tfen 'documents' klasÃ¶rÃ¼ne belge ekleyin.</p>"

        progress(0.3, desc="Belgeler iÅŸleniyor ve indeksleniyor...")
        rag_system.add_documents_to_knowledge_base(documents, force_rebuild=True)
        rag_system.initialize_llm_chain() # LLM'i bilgi bankasÄ± gÃ¼ncellendikten sonra yeniden baÅŸlat
        progress(1, desc="Tamamlama")
        return f"<p style='color: green;'>âœ… Bilgi bankasÄ± baÅŸarÄ±yla yeniden oluÅŸturuldu! ({len(rag_system.documents_for_bm25)} gÃ¼ncel child belge parÃ§asÄ±)</p>"
    except Exception as e:
        logger.error(f"âŒ Tam yeniden oluÅŸturma baÅŸarÄ±sÄ±z oldu: {e}", exc_info=True)
        return f"<p style='color: red;'>âŒ Hata: {str(e)}</p>"

def update_knowledge_base_incremental(progress=gr.Progress()):
    """Bilgi bankasÄ±nÄ± artÄ±mlÄ± olarak gÃ¼nceller."""
    progress(0, desc="Yeni veya gÃ¼ncellenmiÅŸ belgeler aranÄ±yor...")
    logger.info("Bilgi bankasÄ± artÄ±mlÄ± gÃ¼ncelleme baÅŸlatÄ±ldÄ±.")
    try:
        current_files = []
        for root, _, files in os.walk(config.documents_dir): # documents_dir kullan
            for file in files:
                file_path = os.path.join(root, file)
                # UTF-8 BOM'dan temizle
                if file.endswith(('.txt', '.md')): # Sadece metin dosyalarÄ±nÄ± temizle
                    try:
                        convert_to_utf8_no_bom(file_path)
                    except Exception as e:
                        logger.warning(f"UTF-8 BOM temizlenirken hata oluÅŸtu {file_path}: {e}")
                current_files.append(file_path)

        new_or_modified_documents = []
        for file_path in current_files:
            file_hash = rag_system._get_file_hash(file_path)
            last_modified = os.path.getmtime(file_path)

            if file_path not in rag_system.processed_files_meta or \
               rag_system.processed_files_meta[file_path]["hash"] != file_hash or \
               rag_system.processed_files_meta[file_path]["last_modified"] < last_modified:
                
                logger.info(f"Yeni veya gÃ¼ncellenmiÅŸ belge tespit edildi: {os.path.basename(file_path)}")
                if file_path.endswith('.pdf'):
                    loader = PyPDFLoader(file_path)
                elif file_path.endswith(('.txt', '.md')):
                    loader = TextLoader(file_path, encoding='utf-8')
                else:
                    logger.warning(f"Desteklenmeyen dosya tÃ¼rÃ¼ atlanÄ±yor: {file_path}")
                    continue
                
                try:
                    loaded_docs = loader.load()
                    for doc in loaded_docs:
                        doc.metadata["source"] = file_path # Kaynak yolunu metadata'ya ekle
                    new_or_modified_documents.extend(loaded_docs)
                except Exception as e:
                    logger.error(f"Dosya yÃ¼klenirken hata oluÅŸtu {file_path}: {e}")
                    continue
        
        if not new_or_modified_documents:
            logger.info("HiÃ§bir yeni veya gÃ¼ncellenmiÅŸ belge bulunamadÄ±. ArtÄ±mlÄ± gÃ¼ncelleme gerekli deÄŸil.")
            return "<p style='color: blue;'>â„¹ï¸ HiÃ§bir yeni veya gÃ¼ncellenmiÅŸ belge bulunamadÄ±. Bilgi bankasÄ± gÃ¼ncel.</p>"

        progress(0.3, desc="Yeni belgeler iÅŸleniyor ve indeksleniyor...")
        rag_system.add_documents_to_knowledge_base(new_or_modified_documents, force_rebuild=False)
        rag_system.initialize_llm_chain()
        progress(1, desc="Tamamlama")
        return f"<p style='color: green;'>âœ… Bilgi bankasÄ± artÄ±mlÄ± olarak gÃ¼ncellendi! ({len(rag_system.documents_for_bm25)} gÃ¼ncel child belge parÃ§asÄ±)</p>"
    except Exception as e:
        logger.error(f"âŒ ArtÄ±mlÄ± gÃ¼ncelleme baÅŸarÄ±sÄ±z oldu: {e}", exc_info=True)
        return f"<p style='color: red;'>âŒ Hata: {str(e)}</p>"

def clear_chat():
    """Sohbet geÃ§miÅŸini ve LLM belleÄŸini temizler."""
    rag_system.chat_history_memory.clear()
    logger.info("Sohbet geÃ§miÅŸi temizlendi.")
    return "", [] # Gradio sohbet geÃ§miÅŸini ve metin kutusunu temizle

def create_gradio_interface():
    """Gradio arayÃ¼zÃ¼nÃ¼ oluÅŸturur."""
    with gr.Blocks(title="GeliÅŸmiÅŸ RAG Sistemi") as demo:
        gr.Markdown("# GeliÅŸmiÅŸ RAG Sistemi ğŸ¤–")
        gr.Markdown(
            "Bu sistem, Ã¶zel belgeleriniz Ã¼zerinde sohbet etmenizi saÄŸlayan bir RAG (Retrieval Augmented Generation) uygulamasÄ±dÄ±r. "
            "Arama yapmak ve yanÄ±t Ã¼retmek iÃ§in melez alma (BM25 + VektÃ¶r AramasÄ±), reranking ve RRF kullanÄ±r."
        )

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    height=500,
                    label="Sohbet GeÃ§miÅŸi",
                    avatar_images=(None, "https://raw.githubusercontent.com/gradio-app/gradio/main/assets/logo.png")
                )
                msg = gr.Textbox(
                    label="MesajÄ±nÄ±z",
                    placeholder="Buraya yazÄ±n...",
                    container=True,
                    scale=7
                )
                
                with gr.Row():
                    send_btn = gr.Button("GÃ¶nder", variant="primary", scale=1)
                    clear_btn = gr.Button("Sohbeti Temizle", scale=1)
                
                suggestions_output = gr.Dataset(
                    label="Takip SorularÄ± Ã–nerileri",
                    components=[gr.Textbox(visible=False)],
                    samples=[],
                    type="array"
                )

            with gr.Column(scale=1):
                with gr.Accordion("Bilgi BankasÄ± YÃ¶netimi", open=True):
                    gr.Markdown("#### Belgelerinizi 'documents' klasÃ¶rÃ¼ne yÃ¼kleyin.")
                    kb_status = gr.Markdown("Bilgi bankasÄ± durumu: YÃ¼klendi.")
                    rebuild_btn = gr.Button("Bilgi BankasÄ±nÄ± Yeniden OluÅŸtur (Tam)")
                    update_btn = gr.Button("Bilgi BankasÄ±nÄ± GÃ¼ncelle (ArtÄ±mlÄ±)")
                
                with gr.Accordion("Ayarlar", open=False):
                    llm_model_name = gr.Textbox(
                        label="LLM Modeli (Ollama)",
                        value=config.llm_model,
                        interactive=True
                    )
                    ollama_base_url_input = gr.Textbox(
                        label="Ollama Base URL",
                        value=config.ollama_base_url,
                        interactive=True
                    )
                    embeddings_model_name_input = gr.Textbox(
                        label="Embedding Modeli",
                        value=config.embeddings_model_name,
                        interactive=True
                    )
                    cross_encoder_model_name_input = gr.Textbox(
                        label="Cross-Encoder Modeli",
                        value=config.cross_encoder_model_name,
                        interactive=True
                    )
                    parent_chunk_size_slider = gr.Slider(
                        minimum=500, maximum=4000, value=config.parent_chunk_size,
                        label="Parent Chunk Boyutu", step=100
                    )
                    parent_chunk_overlap_slider = gr.Slider(
                        minimum=0, maximum=1000, value=config.parent_chunk_overlap,
                        label="Parent Chunk Ã‡akÄ±ÅŸmasÄ±", step=50
                    )
                    child_chunk_size_slider = gr.Slider(
                        minimum=50, maximum=1000, value=config.child_chunk_size,
                        label="Child Chunk Boyutu", step=10
                    )
                    child_chunk_overlap_slider = gr.Slider(
                        minimum=0, maximum=200, value=config.child_chunk_overlap,
                        label="Child Chunk Ã‡akÄ±ÅŸmasÄ±", step=5
                    )
                    bm25_top_k_slider = gr.Slider(
                        minimum=1, maximum=20, value=config.bm25_top_k,
                        label="BM25 Top K", step=1
                    )
                    vector_top_k_slider = gr.Slider(
                        minimum=1, maximum=20, value=config.vector_top_k,
                        label="Vector Top K", step=1
                    )
                    rerank_top_n_slider = gr.Slider(
                        minimum=1, maximum=10, value=config.top_k_rerank,
                        label="Rerank Top N", step=1
                    )
                    llm_temperature_slider = gr.Slider(
                        minimum=0.0, maximum=1.0, value=config.llm_temperature,
                        label="LLM SÄ±caklÄ±ÄŸÄ±", step=0.1
                    )
                    max_memory_length_slider = gr.Slider(
                        minimum=1, maximum=20, value=config.max_memory_length,
                        label="Sohbet GeÃ§miÅŸi UzunluÄŸu", step=1
                    )
                    update_params_btn = gr.Button("Parametreleri GÃ¼ncelle")


        # Olay Dinleyicileri
        msg.submit(
            fn=respond,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot, suggestions_output],
            queue=False
        )
        send_btn.click(
            fn=respond,
            inputs=[msg, chatbot],
            outputs=[msg, chatbot, suggestions_output],
            queue=False
        )
        clear_btn.click(
            fn=clear_chat,
            inputs=[],
            outputs=[msg, chatbot, suggestions_output], # suggestions_output'Ä± da temizle
            queue=False
        )
        suggestions_output.click(
            lambda x: x, # TÄ±klanan Ã¶ÄŸenin deÄŸerini dÃ¶ndÃ¼rÃ¼r
            inputs=[suggestions_output],
            outputs=[msg] # TÄ±klanan Ã¶neriyi mesaj kutusuna koyar
        )

        rebuild_btn.click(
            fn=rebuild_knowledge_base_full,
            outputs=kb_status
        )
        update_btn.click(
            fn=update_knowledge_base_incremental,
            outputs=kb_status
        )

        # Ayarlar gÃ¼ncellendiÄŸinde
        update_params_btn.click(
            fn=lambda llm_m, ollama_url, emb_m, cross_m, pc_s, pc_o, cc_s, cc_o, bm25_tk, vec_tk, rerank_tn, llm_temp, max_mem_len: (
                setattr(config, 'llm_model', llm_m),
                setattr(config, 'ollama_base_url', ollama_url),
                setattr(config, 'embeddings_model_name', emb_m),
                setattr(config, 'cross_encoder_model_name', cross_m),
                setattr(config, 'parent_chunk_size', pc_s),
                setattr(config, 'parent_chunk_overlap', pc_o),
                setattr(config, 'child_chunk_size', cc_s),
                setattr(config, 'child_chunk_overlap', cc_o),
                setattr(config, 'bm25_top_k', bm25_tk),
                setattr(config, 'vector_top_k', vec_tk),
                setattr(config, 'top_k_rerank', rerank_tn), # top_k_rerank olarak gÃ¼ncellendi
                setattr(config, 'llm_temperature', llm_temp),
                setattr(config, 'max_memory_length', max_mem_len), # max_memory_length gÃ¼ncellendi
                rag_system.initialize_llm_chain(), # LLM'i yeni sÄ±caklÄ±kla ve bellek uzunluÄŸuyla yeniden baÅŸlat
                rag_system.__init__(), # RAGSystem'i tamamen yeniden baÅŸlat (embedding modelleri iÃ§in)
                "<p style='color: green;'>âœ… Parametreler gÃ¼ncellendi ve sistem yeniden yÃ¼klendi!</p>"
            ),
            inputs=[
                llm_model_name,
                ollama_base_url_input,
                embeddings_model_name_input,
                cross_encoder_model_name_input,
                parent_chunk_size_slider,
                parent_chunk_overlap_slider,
                child_chunk_size_slider,
                child_chunk_overlap_slider,
                bm25_top_k_slider,
                vector_top_k_slider,
                rerank_top_n_slider,
                llm_temperature_slider,
                max_memory_length_slider
            ],
            outputs=kb_status,
            queue=False
        )

    return demo

def main():
    """Ana fonksiyon"""
    try:
        logger.info("ğŸš€ GeliÅŸmiÅŸ RAG Sistemi baÅŸlatÄ±lÄ±yor...")

        demo = create_gradio_interface()

        if demo is None:
            logger.error("âŒ Gradio arayÃ¼zÃ¼ oluÅŸturulamadÄ±!")
            return

        logger.info("ğŸŒ Gradio sunucusu baÅŸlatÄ±lÄ±yor...")
        demo.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            debug=False,
            show_error=True,
            inbrowser=True
        )

    except KeyboardInterrupt:
        logger.info("Sistem kapatÄ±lÄ±yor...")
    except Exception as e:
        logger.critical(f"Kritik hata: {e}", exc_info=True)

if __name__ == "__main__":
    main()
