#This is Dumbest AI !!! Do not Argue. :D
import os
import json
import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import hashlib
import pickle
import shutil # Dosya iÅŸlemleri iÃ§in
import time # Gecikme iÃ§in eklendi
import uuid # EKLENDÄ°: uuid modÃ¼lÃ¼ import edildi

import gradio as gr
import torch
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from rank_bm25 import BM25Okapi
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize

from langchain.text_splitter import RecursiveCharacterTextSplitter, SpacyTextSplitter
from langchain.chains import LLMChain # LLMChain import edildi
from langchain.memory import ConversationBufferWindowMemory
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader
from langchain_ollama import OllamaLLM
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain_core.embeddings import Embeddings

# ParentDocumentRetriever iÃ§in yeni importlar
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore # Basit bir dokÃ¼man deposu

from sentence_transformers import SentenceTransformer, CrossEncoder
from convert_encoding import convert_to_utf8_no_bom

# Logging setup
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# NLTK data download
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
except LookupError:
    logger.warning("NLTK 'punkt' or 'stopwords' not found. Attempting to download...")
    try:
        nltk.download('punkt')
        nltk.download('stopwords')
        logger.info("NLTK 'punkt' and 'stopwords' downloaded successfully.")
    except Exception as e:
        logger.error(f"Failed to download NLTK data: {e}. Some features might be affected.")


# Configuration
@dataclass
class RAGConfig:
    vector_store_dir: str = "vectorstore"
    documents_dir: str = "documents"
    chat_history_file: str = "chat_history.json"
    cache_dir: str = "cache"
    
    # Chunking AyarlarÄ± (Small-to-Big iÃ§in)
    parent_chunk_size: int = 1024 # Daha bÃ¼yÃ¼k ana (parent) parÃ§alar
    parent_chunk_overlap: int = 256
    child_chunk_size: int = 64  # **Daha kÃ¼Ã§Ã¼k varsayÄ±lan deÄŸer: VektÃ¶rleÅŸtirmek iÃ§in daha kÃ¼Ã§Ã¼k (child) parÃ§alar**
    child_chunk_overlap: int = 16 # **Daha kÃ¼Ã§Ã¼k varsayÄ±lan deÄŸer**

    top_k_retrieval: int = 10
    top_k_rerank: int = 5
    temperature: float = 0.1
    max_memory_length: int = 10
    confidence_threshold: float = 0.7
    embedding_batch_size: int = 64 # Embedding batch boyutu
    # Ã–NEMLÄ°: vectorstore_add_batch_size, ChromaDB'nin tek seferde iÅŸleyebileceÄŸi child chunk sayÄ±sÄ± olmalÄ± (yaklaÅŸÄ±k 5000-5461)
    vectorstore_add_batch_size: int = 4000 # VarsayÄ±lan olarak 4000 olarak gÃ¼ncellendi.
    # Processed file metadata'yÄ± tutacak dosya
    processed_files_meta_file: str = os.path.join("cache", "processed_files_meta.json")


config = RAGConfig()

# CUDA Setup
print("ğŸš€ CUDA Test")
print("âœ… CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("ğŸ§  Device:", torch.cuda.get_device_name(torch.cuda.current_device()))
    device = "cuda"
else:
    print("âŒ CUDA not available, CPU kullanÄ±lacak.")
    device = "cpu"
torch.cuda.empty_cache()

# RAG-Fusion (Reciprocal Rank Fusion - RRF) fonksiyonu
def reciprocal_rank_fusion(
    ranked_lists: List[List[Tuple[Document, float]]], k: int = 60
) -> List[Tuple[Document, float]]:
    """
    Reciprocal Rank Fusion (RRF) kullanarak birden fazla sÄ±ralÄ± listeyi birleÅŸtirir.
    Daha dÃ¼ÅŸÃ¼k rank daha iyidir. Ä°lk eleman rank 1.
    """
    fused_scores = {}
    k_rrf = 60.0 # RRF iÃ§in sabir k deÄŸeri (genellikle 60)

    # Her bir sÄ±ralÄ± liste Ã¼zerinde dÃ¶n
    for ranked_list in ranked_lists:
        # Check if ranked_list is empty to avoid errors with enumerate
        if not ranked_list:
            continue
        for rank, (doc, original_score) in enumerate(ranked_list):
            # Benzersiz ID'yi metadatadan al veya iÃ§erikten oluÅŸtur
            doc_id = doc.metadata.get('id')
            if doc_id is None:
                doc_id = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()
                doc.metadata['id'] = doc_id # ID'yi belgeye ekle
            
            if doc_id not in fused_scores:
                fused_scores[doc_id] = {'doc': doc, 'score': 0.0}
            
            # RRF formÃ¼lÃ¼: 1 / (k_rrf + rank)
            fused_scores[doc_id]['score'] += 1.0 / (k_rrf + rank + 1) # Rank 0'dan baÅŸladÄ±ÄŸÄ± iÃ§in +1

    # Fused skorlara gÃ¶re sÄ±rala
    sorted_results = sorted(fused_scores.values(), key=lambda x: x['score'], reverse=True)
    
    # Orjinal Document objelerini ve fÃ¼zyon skorlarÄ±nÄ± dÃ¶ndÃ¼r
    return [(item['doc'], item['score']) for item in sorted_results[:k]]


class AdvancedEmbeddings(Embeddings):
    """GeliÅŸmiÅŸ embedding sÄ±nÄ±fÄ± - cache ve batch processing ile"""

    def __init__(self, model_name: str = "nomic-ai/nomic-embed-text-v1"): # nomic-ai modeli varsayÄ±lan
        self.model_name = model_name
        self.cache_file = os.path.join(config.cache_dir, "embeddings_cache.pkl")
        self.cache = self._load_cache()

        try:
            self.model = SentenceTransformer(
                model_name,
                device=device,
                trust_remote_code=True
            )
            logger.info(f"âœ… Embedding model loaded on {device}")
        except Exception as e:
            logger.error(f"âŒ Model loading failed: {e}. Fallback to CPU.")
            self.model = SentenceTransformer(
                model_name,
                device="cpu",
                trust_remote_code=True
            )
            logger.info("âœ… Fallback to CPU mode.")

    def _load_cache(self) -> Dict:
        os.makedirs(config.cache_dir, exist_ok=True)
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'rb') as f:
                    return pickle.load(f)
            except Exception as e: # Catch specific exception
                logger.warning(f"âŒ Embedding cache could not be loaded from {self.cache_file} ({e}). Starting with empty cache.")
                return {}
        return {}

    def _save_cache(self):
        try:
            with open(self.cache_file, 'wb') as f:
                pickle.dump(self.cache, f)
        except Exception as e:
            logger.error(f"âŒ Failed to save embedding cache: {e}")

    def _get_cache_key(self, text: str) -> str:
        return hashlib.md5(text.encode('utf-8')).hexdigest() # encoding belirtildi

    def embed_documents(self, texts: List[str], batch_size: int = None) -> List[List[float]]:
        # config.embedding_batch_size'Ä± kullan
        if batch_size is None:
            batch_size = config.embedding_batch_size

        if not texts: 
            logger.warning("AdvancedEmbeddings.embed_documents received an empty list of texts. Returning empty embeddings.")
            return []

        embeddings = []
        uncached_texts = []
        uncached_indices = []

        # Cache kontrolÃ¼
        for i, text in enumerate(texts):
            cache_key = self._get_cache_key(text) 
            if cache_key in self.cache:
                embeddings.append(self.cache[cache_key])
            else:
                embeddings.append(None) # Yer tutucu
                uncached_texts.append(text)
                uncached_indices.append(i)

        # Yeni embeddingleri oluÅŸtur
        if uncached_texts:
            logger.info(f"ğŸ”„ Embedding {len(uncached_texts)} new documents (in batches of {batch_size})...")
            new_embeddings_list = [] # Batch'lerden gelen embedding'leri toplamak iÃ§in
            
            # tqdm ile ilerleme Ã§ubuÄŸu ekleyelim
            from tqdm import tqdm
            # Ä°ÅŸlemciye uygun sayÄ±da worker kullanma (Windows'da sorunlu olabilir, None bÄ±rakÄ±labilir)
            num_workers = 0 # Default to 0 for Windows compatibility, or os.cpu_count() - 1 for Linux/macOS
            
            for i in tqdm(range(0, len(uncached_texts), batch_size), desc="Embedding Batches"):
                batch = uncached_texts[i:i+batch_size]
                batch_embeddings = self.model.encode(
                    batch,
                    show_progress_bar=False, # tqdm kendi progress barÄ±nÄ± kullanacak
                    convert_to_numpy=True,
                    normalize_embeddings=True,
                    device=self.model.device, # Modelin Ã§alÄ±ÅŸtÄ±ÄŸÄ± cihazÄ± belirt
                    num_workers=num_workers # Batch yÃ¼kleme iÃ§in worker sayÄ±sÄ±
                )
                new_embeddings_list.extend(batch_embeddings)

            # Cache'e kaydet ve orijinal listeye yerleÅŸtir
            for i, text_idx in enumerate(uncached_indices):
                text = uncached_texts[i]
                embedding_data = new_embeddings_list[i].tolist() # Numpy array'i listeye dÃ¶nÃ¼ÅŸtÃ¼r
                cache_key = self._get_cache_key(text)
                self.cache[cache_key] = embedding_data
                embeddings[text_idx] = embedding_data

            self._save_cache()
        else:
            logger.info("âœ… All embeddings found in cache, skipping embedding generation.")

        # None deÄŸerlerini gerÃ§ek embeddingler ile deÄŸiÅŸtir (placeholder kullanÄ±yorsanÄ±z)
        final_embeddings = []
        for emb in embeddings:
            if emb is None:
                logger.error("Hata: Bir embedding boÅŸ kaldÄ±, bu olmamalÄ±ydÄ±. BoÅŸ string embedding'i oluÅŸturuluyor.")
                # Bu durum olmamalÄ±ydÄ± ama olursa boÅŸ bir embedding ile devam et
                final_embeddings.append(self.model.encode("", convert_to_numpy=True, normalize_embeddings=True, device=self.model.device).tolist())
            else:
                final_embeddings.append(emb)

        # Final check before returning: If input texts were not empty but embeddings came out empty
        if not final_embeddings and texts: 
             logger.error("Hata: Girdi metinleri boÅŸ olmamasÄ±na raÄŸmen AdvancedEmbeddings boÅŸ embedding listesi dÃ¶ndÃ¼rdÃ¼. Bu, modelin veya girdinin sorunlu olduÄŸunu gÃ¶sterir.")
        
        return final_embeddings

    def embed_query(self, text: str) -> List[float]:
        cache_key = self._get_cache_key(text)
        if cache_key in self.cache:
            return self.cache[cache_key]

        embedding = self.model.encode(text, convert_to_numpy=True, normalize_embeddings=True, device=self.model.device)
        self.cache[cache_key] = embedding.tolist()
        self._save_cache()
        return embedding.tolist()

class SemanticChunker:
    """Semantik olarak anlamlÄ± parÃ§alara bÃ¶len sÄ±nÄ±f"""

    def __init__(self, embeddings: AdvancedEmbeddings, threshold: float = 0.75):
        self.embeddings = embeddings
        self.threshold = threshold

    def split_text(self, text: str, metadata: Dict = None) -> List[Document]:
        sentences = sent_tokenize(text)
        if len(sentences) < 2:
            return [Document(page_content=text, metadata=metadata or {})]

        # CÃ¼mle embeddingleri (burada kendi embedding modelini kullanmalÄ±)
        sentence_embeddings = self.embeddings.model.encode(sentences, convert_to_numpy=True, device=self.embeddings.model.device)

        # Benzerlik hesapla
        chunks = []
        current_chunk = [sentences[0]]

        for i in range(1, len(sentences)):
            # KosinÃ¼s benzerliÄŸi hesaplamak iÃ§in normalize edilmiÅŸ vektÃ¶rler Ã¶nemlidir
            similarity = cosine_similarity(
                [sentence_embeddings[i-1]],
                [sentence_embeddings[i]]
            )[0][0]

            if similarity > self.threshold:
                current_chunk.append(sentences[i])
            else:
                # Yeni chunk baÅŸlat
                chunk_text = " ".join(current_chunk)
                chunks.append(Document(
                    page_content=chunk_text,
                    metadata={**(metadata or {}), "chunk_type": "semantic"}
                ))
                current_chunk = [sentences[i]]

        # Son chunk'Ä± ekle
        if current_chunk:
            chunk_text = " ".join(current_chunk)
            chunks.append(Document(
                page_content=chunk_text,
                metadata={**(metadata or {}), "chunk_type": "semantic"}
            ))

        return chunks

class HybridRetriever:
    """Hibrit arama: Semantic + Keyword-based"""

    def __init__(self, parent_retriever: ParentDocumentRetriever, bm25_documents: List[Document]):
        self.parent_retriever = parent_retriever # ParentDocumentRetriever instance'Ä±
        self.bm25_documents = bm25_documents # BM25 iÃ§in tÃ¼m child dokÃ¼manlarÄ±
        # Self.embeddings'i kaldÄ±rdÄ±k Ã§Ã¼nkÃ¼ reranker direkt olarak modelini init'te alÄ±yor.
        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device) # Reranker'Ä± da GPU'ya taÅŸÄ±
        
        # Stop words: BaÅŸlangÄ±Ã§ta boÅŸ bir set ile gÃ¼venli baÅŸlatma
        self.stop_words = set() 
        try:
            # TÃ¼rkÃ§e ve Ä°ngilizce stop word'leri birleÅŸtirme
            nltk_stop_words = set(stopwords.words('english'))
            nltk_stop_words.update(stopwords.words('turkish'))
            self.stop_words = nltk_stop_words
        except LookupError:
            logger.warning("NLTK stopwords could not be loaded. Running without stopwords.")
        
        # BM25 indeksi oluÅŸtur
        self.bm25 = self._build_bm25_index()


    def _build_bm25_index(self):
        """BM25 indeksi oluÅŸtur"""
        # Sadece BM25 iÃ§in belge iÃ§eriklerini tokenize et
        corpus = []
        for doc in self.bm25_documents: # BM25 iÃ§in kullanÄ±lan dokÃ¼man listesi
            tokens = word_tokenize(doc.page_content.lower())
            tokens = [t for t in tokens if t.isalnum() and t not in self.stop_words] 
            corpus.append(tokens)

        if not corpus:
            logger.warning("BM25 index cannot be built: No valid document content found for tokenization.")
            return None
        return BM25Okapi(corpus)

    def _preprocess_query(self, query: str) -> str:
        """Query'yi zenginleÅŸtir"""
        expanded_terms = []
        tokens = word_tokenize(query.lower())

        for token in tokens:
            if token.isalnum() and token not in self.stop_words:
                expanded_terms.append(token)
                if token in ["nasÄ±l", "how"]:
                    expanded_terms.extend(["yÃ¶ntem", "method", "way"])
                elif token in ["nedir", "what"]:
                    expanded_terms.extend(["tanÄ±m", "definition", "aÃ§Ä±klama"])
        return " ".join(expanded_terms)

    def retrieve(self, query: str, k: int = 10) -> List[Tuple[Document, float]]:
        """Hibrit retrieval (RRF ile)"""
        processed_query = self._preprocess_query(query)

        # 1. Semantic search (ParentDocumentRetriever'Ä±n underlying vectorstore'dan child chunks al)
        # ParentDocumentRetriever'Ä±n child retriever'Ä±nÄ± kullanarak skorlu child chunk'larÄ± al
        semantic_child_results = self.parent_retriever.vectorstore.similarity_search_with_score(
            processed_query, k=k*3 # RRF ve reranking iÃ§in daha fazla al
        )
        
        # 2. BM25 search (child dokÃ¼manlar Ã¼zerinde)
        bm25_child_results = []
        if self.bm25 is not None:
            query_tokens = word_tokenize(processed_query.lower())
            query_tokens = [t for t in query_tokens if t.isalnum() and t not in self.stop_words]
            if query_tokens:
                bm25_scores = self.bm25.get_scores(query_tokens)
                # BM25 sonuÃ§larÄ±nÄ± (document, score) formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
                # Her dokÃ¼manÄ±n BM25 skoru ile eÅŸleÅŸtirilmesi gerekir
                temp_bm25_results = []
                for i, score in enumerate(bm25_scores):
                    if i < len(self.bm25_documents):
                        temp_bm25_results.append((self.bm25_documents[i], float(score)))
                
                temp_bm25_results.sort(key=lambda x: x[1], reverse=True) # Skorlara gÃ¶re sÄ±rala
                bm25_child_results = temp_bm25_results[:k*3] # RRF iÃ§in daha fazla al
            else:
                logger.warning("BM25 search skipped: Preprocessed query has no valid tokens.")
        else:
            logger.warning("BM25 index is not available. Performing only semantic search.")

        # RRF kullanarak sonuÃ§larÄ± birleÅŸtir
        # Her iki liste de (Document, score) formatÄ±nda olmalÄ± ve Document.metadata['id'] iÃ§ermeli.
        fused_child_results = reciprocal_rank_fusion([semantic_child_results, bm25_child_results], k=k) # top_k kadar sonuÃ§ al
        
        return fused_child_results


    def rerank_results(self, query: str, results: List[Tuple[Document, float]], top_k: int = 5) -> List[Tuple[Document, float]]:
        """Cross-encoder ile reranking"""
        if len(results) <= top_k or not results:
            return results

        # Reranking iÃ§in query-document pairs hazÄ±rla
        # context iÃ§in sadece page_content alÄ±yoruz.
        pairs = [(query, doc.page_content) for doc, _ in results]

        try:
            rerank_scores = self.reranker.predict(pairs)

            reranked_results = [
                (results[i][0], float(score)) # Orijinal dokÃ¼man objesini kullan
                for i, score in enumerate(rerank_scores)
            ]
            reranked_results.sort(key=lambda x: x[1], reverse=True)

            return reranked_results[:top_k]
        except Exception as e:
            logger.warning(f"Reranking failed: {e}. Returning original top-k results.")
            return results[:top_k]

class AdvancedRAGSystem:
    """GeliÅŸmiÅŸ RAG sistemi"""

    def __init__(self):
        self.embeddings = AdvancedEmbeddings() # Initialized here
        self.docstore = InMemoryStore() # ParentDocumentRetriever iÃ§in dokÃ¼man deposu
        self.vectorstore = None # Child chunks iÃ§in Chroma
        self.parent_document_retriever = None # Langchain ParentDocumentRetriever
        self.hybrid_retriever = None # Kendi HybridRetriever'Ä±mÄ±z
        self.documents_for_bm25 = [] # BM25 iÃ§in kullanÄ±lan tÃ¼m child dokÃ¼manlarÄ± (metadata iÃ§erir)

        self.llm = None
        self.memory = None
        self.qa_chain = None # Åimdi bir LLMChain olacak
        self.response_cache = {}
        self.processed_files_meta = self._load_processed_files_meta()

    def _get_file_hash(self, file_path: str) -> str:
        """DosyanÄ±n MD5 hash'ini hesaplar."""
        hasher = hashlib.md5()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b''):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except Exception as e:
            logger.error(f"âŒ Dosya hash'i alÄ±namadÄ± {file_path}: {e}")
            return ""

    def _load_processed_files_meta(self) -> Dict[str, Dict]:
        """Ä°ÅŸlenmiÅŸ dosyalarÄ±n metadata'sÄ±nÄ± yÃ¼kler."""
        if os.path.exists(config.processed_files_meta_file):
            try:
                with open(config.processed_files_meta_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"âŒ Processed files metadata could not be loaded: {e}. Starting fresh.")
        return {}

    def _save_processed_files_meta(self):
        """Ä°ÅŸlenmiÅŸ dosyalarÄ±n metadata'sÄ±nÄ± kaydeder."""
        os.makedirs(os.path.dirname(config.processed_files_meta_file), exist_ok=True)
        try:
            with open(config.processed_files_meta_file, 'w', encoding='utf-8') as f:
                json.dump(self.processed_files_meta, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"âŒ Failed to save processed files metadata: {e}")

    def build_knowledge_base(self, force_rebuild: bool = False):
        """GeliÅŸmiÅŸ knowledge base oluÅŸtur veya artÄ±mlÄ± olarak gÃ¼ncelle"""
        os.makedirs(config.documents_dir, exist_ok=True)
        os.makedirs(config.vector_store_dir, exist_ok=True)
        os.makedirs(config.cache_dir, exist_ok=True) # Cache dizini de oluÅŸmalÄ±

        if force_rebuild:
            logger.info("ğŸ”„ Force rebuild requested. Deleting existing vectorstore, cache, and metadata.")
            
            # Explicitly clear references to potentially locked resources
            self.vectorstore = None
            self.parent_document_retriever = None
            self.hybrid_retriever = None
            # Ã–nemli: self.embeddings'i yeniden baÅŸlatmak, eski Ã¶nbellek dosyasÄ±nÄ±n bÄ±rakÄ±lmasÄ±na yardÄ±mcÄ± olur.
            self.embeddings = AdvancedEmbeddings() 
            
            # Allow a small delay for OS to release file handles (optional, but can help)
            time.sleep(0.1) 

            if os.path.exists(config.vector_store_dir):
                try:
                    shutil.rmtree(config.vector_store_dir)
                    logger.info(f"âœ… Deleted existing vectorstore directory: {config.vector_store_dir}")
                except OSError as e:
                    logger.error(f"âŒ Could not delete vectorstore directory '{config.vector_store_dir}': {e}. Please ensure no other process is using these files and try again.")
                    raise # Re-raise the error to stop the process if deletion fails

            if os.path.exists(config.cache_dir):
                try:
                    shutil.rmtree(config.cache_dir)
                    logger.info(f"âœ… Deleted existing cache directory: {config.cache_dir}")
                except OSError as e:
                    logger.error(f"âŒ Could not delete cache directory '{config.cache_dir}': {e}. Please ensure no other process is using these files and try again.")
                    # Don't re-raise for cache if vectorstore deletion already worked or is main concern
            
            self.processed_files_meta = {}
            self._save_processed_files_meta()
            self.docstore = InMemoryStore() # Yeni bir InMemoryStore instance'Ä± oluÅŸtur

        # Chroma'yÄ± yÃ¼kle veya boÅŸ oluÅŸtur (child chunks iÃ§in)
        # Chroma'nÄ±n dizini yoksa hata verebilir, ensure_directory_exists ile kontrol edelim.
        if not os.path.exists(config.vector_store_dir):
            os.makedirs(config.vector_store_dir)

        self.vectorstore = Chroma(
            persist_directory=config.vector_store_dir,
            embedding_function=self.embeddings
        )

        # ParentDocumentRetriever'Ä± initialize et
        # Bu Retriever'Ä± artÄ±k sadece child_splitter ve parent_splitter'Ä± kullanmak iÃ§in tutuyoruz.
        # add_documents metodunu artÄ±k doÄŸrudan Ã§aÄŸÄ±rmÄ±yoruz, Ã§Ã¼nkÃ¼ manuel olarak yÃ¶neteceÄŸiz.
        self.parent_document_retriever = ParentDocumentRetriever(
            vectorstore=self.vectorstore, # Placeholder, aslÄ±nda manuel olarak ekleyeceÄŸiz
            docstore=self.docstore,
            child_splitter=RecursiveCharacterTextSplitter(chunk_size=config.child_chunk_size, chunk_overlap=config.child_chunk_overlap),
            parent_splitter=RecursiveCharacterTextSplitter(chunk_size=config.parent_chunk_size, chunk_overlap=config.parent_chunk_overlap),
        )


        logger.info("ğŸ” Checking for document changes...")
        convert_to_utf8_no_bom(config.documents_dir)

        current_files_in_dir = set()
        new_or_modified_original_documents_to_add = [] 
        
        # 1. Mevcut belgeleri tara ve deÄŸiÅŸiklikleri belirle
        for root, _, filenames in os.walk(config.documents_dir):
            for filename in filenames:
                file_path = os.path.join(root, filename)
                relative_file_path = os.path.relpath(file_path, config.documents_dir)
                current_files_in_dir.add(relative_file_path)

                current_hash = self._get_file_hash(file_path)
                last_processed_meta = self.processed_files_meta.get(relative_file_path)

                if not last_processed_meta or last_processed_meta['hash'] != current_hash:
                    logger.info(f"ğŸ†•/ğŸ“ Found new or modified document: {relative_file_path}")
                    
                    original_docs_from_file = self._load_raw_document(file_path)
                    if original_docs_from_file:
                        logger.info(f"Successfully loaded {len(original_docs_from_file)} raw document(s) from {relative_file_path}.")
                        for doc in original_docs_from_file:
                            doc.metadata['id'] = hashlib.md5(relative_file_path.encode('utf-8')).hexdigest()
                            doc.metadata['original_file_path'] = relative_file_path
                            new_or_modified_original_documents_to_add.append(doc)
                    else:
                        logger.warning(f"âš ï¸ Could not load original document: {relative_file_path}. It might be empty, unreadable, or an unsupported format.")
                    
                    self.processed_files_meta[relative_file_path] = {
                        'hash': current_hash,
                        'last_processed': datetime.now().isoformat(),
                    }
                else:
                    pass # Already processed and not modified, no action needed for original documents


        # 2. Silinen belgeleri belirle ve docstore/vectorstore'dan kaldÄ±r
        deleted_file_paths = []
        for processed_file_path, meta in list(self.processed_files_meta.items()):
            if processed_file_path not in current_files_in_dir:
                logger.info(f"ğŸ—‘ï¸ Found deleted document (metadata will be removed): {processed_file_path}")
                parent_id_to_delete = hashlib.md5(processed_file_path.encode('utf-8')).hexdigest()
                
                # Docstore'dan kaldÄ±r
                if hasattr(self.docstore, 'delete') and parent_id_to_delete in self.docstore.docs:
                    self.docstore.delete([parent_id_to_delete])
                    logger.info(f"Deleted parent document {parent_id_to_delete} from docstore.")
                
                # Vectorstore'dan ilgili child chunk'larÄ± kaldÄ±r
                # Bu, ChromaDB'nin delete metodunu kullanarak metadata filtrelemesiyle yapÄ±labilir.
                try:
                    self.vectorstore._collection.delete(where={"parent_id": parent_id_to_delete})
                    logger.info(f"Deleted child chunks for parent_id {parent_id_to_delete} from vectorstore.")
                except Exception as e:
                    logger.error(f"âŒ Error deleting child chunks from vectorstore for {parent_id_to_delete}: {e}")

                deleted_file_paths.append(processed_file_path)
                del self.processed_files_meta[processed_file_path]
        
        if deleted_file_paths:
            logger.warning(f"Note: {len(deleted_file_paths)} deleted files processed.")

        # 3. Yeni/DeÄŸiÅŸtirilmiÅŸ orijinal belgeleri manuel olarak iÅŸleyip Chroma'ya ekle
        if new_or_modified_original_documents_to_add:
            total_original_docs_to_add = len(new_or_modified_original_documents_to_add)
            logger.info(f"â• Processing {total_original_docs_to_add} new/modified original documents for vectorstore.")

            all_new_child_chunks = []
            
            # Parent dokÃ¼manlarÄ± parÃ§ala ve docstore'a ekle, Ã§ocuk chunk'larÄ± topla
            for i, original_doc in enumerate(new_or_modified_original_documents_to_add):
                logger.info(f"    Chunking original document {i+1}/{total_original_docs_to_add}: {original_doc.metadata.get('original_file_path', 'N/A')}")
                
                # Parent dokÃ¼manÄ± parent_splitter ile parÃ§ala
                parent_chunks_from_original_doc = self.parent_document_retriever.parent_splitter.split_documents([original_doc])
                
                for parent_piece_of_original_doc in parent_chunks_from_original_doc:
                    # Yeni bir ID atayalÄ±m, Ã§Ã¼nkÃ¼ bu aslÄ±nda bir "alt-parent" parÃ§a oluyor orijinal belge iÃ§in
                    # Bu ID, ParentDocumentRetriever'Ä±n internal parent_id'si gibi davranacak
                    current_parent_piece_id = str(uuid.uuid4()) # Her parent parÃ§asÄ±na yeni bir ID
                    parent_piece_of_original_doc.metadata['id'] = current_parent_piece_id # Metadata'ya ekle
                    parent_piece_of_original_doc.metadata['original_file_path'] = original_doc.metadata['original_file_path']

                    # DÃ¼zeltme: mset metodu key-value tuple'larÄ±ndan oluÅŸan bir liste bekler
                    self.docstore.mset([(current_parent_piece_id, parent_piece_of_original_doc)]) 
                    
                    # Bu parent parÃ§ayÄ± child_splitter ile parÃ§ala
                    child_chunks_from_parent_piece = self.parent_document_retriever.child_splitter.split_documents([parent_piece_of_original_doc])
                    
                    for child_chunk in child_chunks_from_parent_piece:
                        # Child chunk'a orijinal dosya yolunu ve parent ID'yi ekle
                        child_chunk.metadata['original_file_path'] = original_doc.metadata['original_file_path']
                        child_chunk.metadata['parent_id'] = current_parent_piece_id # Bu child'Ä±n ait olduÄŸu parent parÃ§anÄ±n ID'si
                        child_chunk.metadata['parent_doc_id'] = current_parent_piece_id # Langchain'in beklediÄŸi anahtar
                        all_new_child_chunks.append(child_chunk)
            
            # Åimdi tÃ¼m toplanan child chunk'larÄ± kÃ¼Ã§Ã¼k partiler halinde Chroma'ya ekle
            if all_new_child_chunks:
                logger.info(f"Total {len(all_new_child_chunks)} child chunks generated. Adding to vectorstore in batches of {config.vectorstore_add_batch_size}.")
                for i in range(0, len(all_new_child_chunks), config.vectorstore_add_batch_size):
                    sub_batch = all_new_child_chunks[i:i + config.vectorstore_add_batch_size]
                    
                    try:
                        self.vectorstore.add_documents(sub_batch)
                        logger.info(f"    Added batch {int(i/config.vectorstore_add_batch_size) + 1}/{(len(all_new_child_chunks) + config.vectorstore_add_batch_size - 1) // config.vectorstore_add_batch_size} to ChromaDB.")
                    except Exception as e:
                        logger.error(f"âŒ Error adding child chunks batch to ChromaDB: {e}. Batch size was {len(sub_batch)}")
                        raise

            logger.info("âœ… Vectorstore updated with new/modified documents.")
        else:
            logger.info("â„¹ï¸ No new or modified original documents to add to vectorstore (all are up-to-date or no files found).")
        
        # BM25 iÃ§in gÃ¼ncel documents_for_bm25 listesini doldur (tÃ¼m child chunks)
        # Bu kÄ±sÄ±m, tÃ¼m child chunk'larÄ± yeniden yÃ¼kleyerek gÃ¼ncel listeyi oluÅŸturur.
        # Bu, Ã¶nceki ekleme/silme iÅŸlemlerinden sonra tutarlÄ±lÄ±ÄŸÄ± saÄŸlar.
        self.documents_for_bm25 = []
        # TÃ¼m iÅŸlenmiÅŸ dosyalarÄ±n metadatalarÄ± Ã¼zerinden geÃ§erek child chunk'larÄ± yeniden yÃ¼kle
        # Bu, `processed_files_meta`'da olan tÃ¼m dosyalarÄ±n BM25 iÃ§in yeniden yÃ¼klenmesini saÄŸlar.
        for file_path_rel, meta in list(self.processed_files_meta.items()): # list() Ã§aÄŸrÄ±sÄ±, dÃ¶ngÃ¼ sÄ±rasÄ±nda dict deÄŸiÅŸirse sorun olmamasÄ± iÃ§in
            full_file_path = os.path.join(config.documents_dir, file_path_rel)
            # _load_single_document_and_chunk zaten child chunk'lar Ã¼retip dÃ¶ndÃ¼rÃ¼yor.
            loaded_chunks_for_bm25 = self._load_single_document_and_chunk(full_file_path)
            if loaded_chunks_for_bm25:
                # logger.info(f"Loaded {len(loaded_chunks_for_bm25)} child chunks for BM25 from {file_path_rel}.") # Ã‡ok fazla log olabilir
                self.documents_for_bm25.extend(loaded_chunks_for_bm25)
            else:
                logger.warning(f"âš ï¸ Could not re-load chunks for BM25 from {full_file_path}. Document might be missing, unreadable, or produced no chunks after splitting.")


        if not self.documents_for_bm25:
            logger.warning("No documents loaded into self.documents_for_bm25 for BM25 retriever, keyword search may not function correctly.")

        # HybridRetriever'Ä± yeniden initialize et
        self.hybrid_retriever = HybridRetriever(self.parent_document_retriever, self.documents_for_bm25)

        self._save_processed_files_meta()

        logger.info(f"âœ… Knowledge base updated. Total {len(self.documents_for_bm25)} child chunks for BM25.")
        return self.vectorstore

    def _load_raw_document(self, file_path: str) -> List[Document]:
        """Tek bir orijinal belgeyi parÃ§alamadan yÃ¼kler."""
        try:
            ext = os.path.splitext(file_path)[1].lower()
            loader = None
            
            if ext == '.pdf':
                loader = PyPDFLoader(file_path)
            elif ext in ['.txt', '.md', '.py', '.cs', '.json', '.xml', '.html', '.css', '.js', '.ts', '.tsx', '.jsx', '.go', '.java', '.php', '.rb', '.swift', '.kt', '.c', '.cpp', '.h', '.hpp', '.scss', '.cshtml']:
                loader = TextLoader(file_path, encoding="utf-8")
            else:
                logger.warning(f"âš ï¸ Unsupported file type for loading: '{ext}' in {file_path}. Skipping.")
                return []

            documents = loader.load()
            logger.info(f"Loaded {len(documents)} documents from {file_path} using {type(loader).__name__}.")
            
            if not documents:
                logger.warning(f"Loader returned an empty document list for {file_path}. File might be empty, corrupted, or unreadable.")
                return []

            valid_documents = []
            for doc in documents:
                if doc.page_content.strip():
                    valid_documents.append(doc)
                    logger.info(f"Document from {file_path} (page {doc.metadata.get('page', 'N/A')}): content length = {len(doc.page_content)} characters.")
                else:
                    logger.warning(f"Document from {file_path} (page {doc.metadata.get('page', 'N/A')}) has empty or whitespace-only content after loading. Skipping.")

            if not valid_documents:
                logger.warning(f"All documents loaded from {file_path} were empty or contained only whitespace. Returning empty list.")
                return []

            for doc in valid_documents:
                doc.metadata.update({
                    'file_type': ext,
                    'file_size': os.path.getsize(file_path) if os.path.exists(file_path) else 0,
                    'created_at': datetime.now().isoformat(),
                    'source': file_path
                })
            return valid_documents
        except Exception as e:
            logger.warning(f"âš ï¸ Hata: Orijinal belge yÃ¼klenemedi {file_path}: {e}")
            return []

    def _load_single_document_and_chunk(self, file_path: str) -> List[Document]:
        """
        Tek bir belgeyi yÃ¼kler ve child chunk olarak parÃ§alar.
        """
        try:
            raw_docs = self._load_raw_document(file_path)
            if not raw_docs:
                logger.warning(f"No raw documents to chunk for BM25 from {file_path}. Skipping chunking for this file.")
                return []

            child_splitter = RecursiveCharacterTextSplitter(
                chunk_size=config.child_chunk_size,
                chunk_overlap=config.child_chunk_overlap
            )
            chunks = child_splitter.split_documents(raw_docs)

            if not chunks:
                logger.warning(f"Child splitter produced no chunks for {file_path} with chunk_size {config.child_chunk_size}. Document content might be too small or contains unchunkable content.")
                return []

            file_base_hash = hashlib.md5(os.path.relpath(file_path, config.documents_dir).encode('utf-8')).hexdigest()
            
            processed_chunks = [] 
            for i, chunk in enumerate(chunks):
                if chunk.page_content.strip():
                    chunk.metadata['id'] = f"{file_base_hash}_{i}"
                    chunk.metadata['original_file_path'] = os.path.relpath(file_path, config.documents_dir)
                    # Buradaki parent_id, orijinal dosyanÄ±n ID'si olmalÄ±
                    chunk.metadata['parent_id'] = file_base_hash 
                    processed_chunks.append(chunk)
                else:
                    logger.warning(f"Skipping empty or whitespace-only chunk generated from {file_path} (original chunk index: {i}).")

            if not processed_chunks:
                logger.warning(f"All generated chunks for {file_path} were empty or whitespace-only after content check. Returning empty list.")
            return processed_chunks
        except Exception as e:
            logger.warning(f"âš ï¸ Hata: Belge child chunk olarak parÃ§alanamadÄ± {file_path}: {e}")
            return []

    def initialize_llm_chain(self):
        """LLM chain'i initialize et"""
        self.memory = ConversationBufferWindowMemory(
            memory_key="chat_history",
            return_messages=True,
            k=config.max_memory_length
        )

        self.llm = OllamaLLM(
            model="llama3",
            temperature=config.temperature,
            top_p=0.9,
            repeat_penalty=1.1
        )

        prompt_template = """Sen uzman bir yapay zeka asistansÄ±n ve Ã¶zellikle .NET ve web geliÅŸtirme konularÄ±nda deneyimlisin. GÃ¶revin, aÅŸaÄŸÄ±da verilen 'BAÄLAM BÄ°LGÄ°LERÄ°'ne dayanarak kullanÄ±cÄ±nÄ±n sorularÄ±nÄ± doÄŸru, eksiksiz, detaylÄ± ve aÃ§Ä±klayÄ±cÄ± bir ÅŸekilde yanÄ±tlamaktÄ±r.

BAÄLAM BÄ°LGÄ°LERÄ°:
{context}

SOHBET GEÃ‡MÄ°ÅÄ°:
{chat_history}

KULLERANIICI SORUSU: {question}

YÃ–NERGE:
1. YanÄ±tÄ±nÄ± **Ã¶ncelikle ve aÄŸÄ±rlÄ±klÄ± olarak** 'BAÄLAM BÄ°LGÄ°LERÄ°' iÃ§inde yer alan bilgilere dayanarak oluÅŸtur. Kendi genel bilgilerini kullanmaktan kaÃ§Ä±n, ancak baÄŸlamÄ± daha iyi aÃ§Ä±klamak veya yapÄ±landÄ±rmak iÃ§in yardÄ±mcÄ± olabilirsin.
2. EÄŸer 'BAÄLAM BÄ°LGÄ°LERÄ°'nde soruyu yanÄ±tlamak iÃ§in yeterli veya ilgili bilgi yoksa, 'ÃœzgÃ¼nÃ¼m, bu bilgiyi saÄŸlanan belgelerde yeterince detaylÄ± bulamadÄ±m veya ilgili bir bilgiye rastlamadÄ±m. BaÅŸka bir ÅŸey sormak ister misiniz?' ÅŸeklinde nazikÃ§e belirt ve **asla yanÄ±t UYDURMA**.
3. CevabÄ±nÄ± her zaman 'BAÄLAM BÄ°LGÄ°LERÄ°'ndeki ilgili kaynak numaralarÄ±yla destekle (Ã¶rn: [Kaynak 1], [Kaynak 2]).
4. YanÄ±tlarÄ±nÄ± organize et (maddeler, baÅŸlÄ±klar kullanabilirsin) ve karmaÅŸÄ±k konularÄ± anlaÅŸÄ±lÄ±r bir dille aÃ§Ä±kla. MÃ¼mkÃ¼nse Ã¶rnekler veya senaryolar sun.
5. YanÄ±tÄ±nÄ±n sonunda, cevabÄ±nÄ±n ne kadar gÃ¼venilir olduÄŸunu 0 ile 1 arasÄ±nda bir ondalÄ±k sayÄ± olarak 'GÃ¼venilirlik Skoru:' ÅŸeklinde belirt. (Ã–rn: GÃ¼venilirlik Skoru: 0.95)

YANITIM:"""

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "chat_history", "question"]
        )

        self.qa_chain = LLMChain(llm=self.llm, prompt=prompt, verbose=True)


    def enhanced_query_processing(self, query: str) -> Dict[str, Any]:
        """GeliÅŸmiÅŸ query iÅŸleme"""
        query_hash = hashlib.md5(query.encode('utf-8')).hexdigest()
        if query_hash in self.response_cache:
            logger.info("ğŸ“‹ Using cached response for query.")
            return {**self.response_cache[query_hash], "cached": True}
        
        if self.hybrid_retriever is None:
            return {
                "answer": "ÃœzgÃ¼nÃ¼m, bilgi bankasÄ± henÃ¼z hazÄ±r deÄŸil veya boÅŸ.",
                "sources": [],
                "confidence": 0.0,
                "error": "Retriever is not initialized."
            }

        # Hibrit retrieval (BM25 ve Semantic arama ve RRF birleÅŸtirme)
        retrieved_child_docs_with_scores = self.hybrid_retriever.retrieve(
            query, k=config.top_k_retrieval * 2
        )
        
        # Reranking (hala child dokÃ¼manlar Ã¼zerinde)
        reranked_child_docs_with_scores = self.hybrid_retriever.rerank_results(
            query, retrieved_child_docs_with_scores, top_k=config.top_k_rerank
        )
        
        # Åimdi Small-to-Big adÄ±mÄ±nÄ± uyguluyoruz:
        final_context_docs = []
        unique_parent_ids = set()
        
        for child_doc, score in reranked_child_docs_with_scores:
            parent_id = child_doc.metadata.get('parent_doc_id') # Langchain'in atadÄ±ÄŸÄ± parent_doc_id
            if not parent_id: # Fallback: Bizim manuel atadÄ±ÄŸÄ±mÄ±z parent_id
                parent_id = child_doc.metadata.get('parent_id')

            if parent_id and parent_id not in unique_parent_ids:
                parent_docs_from_store = self.docstore.mget([parent_id]) 
                parent_doc = parent_docs_from_store[0] if parent_docs_from_store and parent_docs_from_store[0] else None

                if parent_doc:
                    parent_doc.metadata['original_file_path'] = child_doc.metadata.get('original_file_path', 'N/A')
                    final_context_docs.append((parent_doc, score))
                    unique_parent_ids.add(parent_id)
                else:
                    logger.warning(f"Parent document with ID {parent_id} not found in docstore for child {child_doc.metadata.get('id', 'N/A')}. Using child document as fallback.")
                    final_context_docs.append((child_doc, score))
            else:
                if parent_id is None:
                     final_context_docs.append((child_doc, score))
                elif parent_id in unique_parent_ids:
                    pass
                else:
                    final_context_docs.append((child_doc, score))

        if not final_context_docs and reranked_child_docs_with_scores:
            logger.warning("No parent documents retrieved. Falling back to reranked child documents for context.")
            final_context_docs = reranked_child_docs_with_scores

        context = "\n\n".join([
            f"[Kaynak {i+1}] {doc.page_content}"
            for i, (doc, score) in enumerate(final_context_docs)
        ])
        
        try:
            chat_history_messages = self.memory.load_memory_variables({})["chat_history"]
            
            prompt_inputs = {
                "question": query,
                "chat_history": chat_history_messages,
                "context": context
            }
            
            result = self.qa_chain.invoke(prompt_inputs)
            answer = result["text"]
            
            self.memory.save_context({"input": query}, {"output": answer})
            
            confidence = self._calculate_confidence(query, final_context_docs, answer)
            
            response = {
                "answer": answer,
                "sources": [
                    {
                        "id": doc.metadata.get('id', 'N/A'),
                        "original_file_path": doc.metadata.get('original_file_path', doc.metadata.get('source', 'N/A')),
                        "content": doc.page_content[:200] + "...",
                        "metadata": doc.metadata,
                        "score": float(score)
                    }
                    for doc, score in final_context_docs
                ],
                "confidence": confidence,
                "timestamp": datetime.now().isoformat()
            }
            
            self.response_cache[query_hash] = response
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Query processing failed: {e}")
            return {
                "answer": "ÃœzgÃ¼nÃ¼m, sorgunuzu iÅŸlerken bir hata oluÅŸtu.",
                "sources": [],
                "confidence": 0.0,
                "error": str(e)
            }

    def _calculate_confidence(self, query: str, docs: List[Tuple[Document, float]], answer: str) -> float:
        """Basit confidence skoru hesapla"""
        if not docs:
            return 0.0
        
        avg_retrieval_score = np.mean([score for _, score in docs])
        
        answer_length_factor = min(1.0, len(answer.split()) / 50.0)
        answer_length_factor = max(0.3, answer_length_factor)

        if avg_retrieval_score < 0.2:
            confidence = 0.1
        else:
            confidence = (avg_retrieval_score * 0.7) + (answer_length_factor * 0.3)
        
        if "bulamadÄ±m" in answer.lower() or "saÄŸlanan belgeler" in answer.lower() or "Ã¼zgÃ¼nÃ¼m" in answer.lower():
            confidence = min(confidence, 0.2)

        return min(1.0, max(0.0, confidence))

    def get_follow_up_suggestions(self, query: str, response: Dict[str, Any]) -> List[str]:
        """Follow-up soru Ã¶nerileri"""
        suggestions = []
        
        query_lower = query.lower()
        
        if "nedir" in query_lower or "what is" in query_lower:
            suggestions.append(f"Bu konuda daha detaylÄ± bilgi verir misin?")
            suggestions.append(f"Ã–rneklerle aÃ§Ä±klar mÄ±sÄ±n?")
        
        if "nasÄ±l" in query_lower or "how" in query_lower:
            suggestions.append(f"Alternatif yÃ¶ntemler nelerdir?")
            suggestions.append(f"AdÄ±m adÄ±m anlatÄ±r mÄ±sÄ±n?")
        
        if "neden" in query_lower or "why" in query_lower:
            suggestions.append(f"Bu durumun sonuÃ§larÄ± nelerdir?")
            suggestions.append(f"BaÅŸka sebepleri var mÄ±?")
        
        sources = response.get("sources", [])
        if sources:
            source_files = list(set([os.path.basename(s.get('original_file_path', '')) for s in sources if s.get('original_file_path')]))
            if source_files:
                suggestions.append(f"{source_files[0]} dosyasÄ±nda baÅŸka hangi bilgiler var?")
                if len(source_files) > 1:
                    suggestions.append(f"{source_files[1]} dosyasÄ±nÄ±n iÃ§eriÄŸini Ã¶zetler misin?")

        return suggestions[:3]

def save_chat_history(history: List[Dict]):
    """Chat history kaydet"""
    try:
        with open(config.chat_history_file, "w", encoding="utf-8") as f:
            json.dump(history, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Failed to save chat history: {e}")

def load_chat_history() -> List[Dict]:
    """Chat history yÃ¼kle"""
    if os.path.exists(config.chat_history_file):
        try:
            with open(config.chat_history_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load chat history: {e}")
            return []
    return []

def create_gradio_interface():
    """GeliÅŸmiÅŸ Gradio arayÃ¼zÃ¼"""

    rag_system = AdvancedRAGSystem()

    logger.info("ğŸ—ï¸ Building knowledge base...")
    vectorstore = rag_system.build_knowledge_base()

    if vectorstore is None:
        logger.error("âŒ Knowledge base creation failed!")
        return None

    logger.info("ğŸ”— Initializing LLM chain...")
    rag_system.initialize_llm_chain()

    chat_history = load_chat_history()

    with gr.Blocks(
        title="ğŸ§  GeliÅŸmiÅŸ RAG Sistemi",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        .message-wrap {
            padding: 15px !important;
        }
        """
    ) as demo:

        gr.Markdown("""
        # ğŸ§  GeliÅŸmiÅŸ RAG (Retrieval-Augmented Generation) Sistemi

        **Ã–zellikler:**
        - ğŸ” Hibrit Arama (Semantic + Keyword)
        - ğŸ¯ AkÄ±llÄ± Reranking (Cross-Encoder)
        - ğŸ’¾ YanÄ±t Cache'i
        - ğŸ“Š GÃ¼venilirlik Skoru
        - ğŸ“š Kaynak GÃ¶sterimi
        - ğŸ’¡ Follow-up Ã–nerileri
        - ğŸš€ ArtÄ±mlÄ± Bilgi BankasÄ± GÃ¼ncelleme
        - âœ¨ **Small-to-Big Chunking**
        - ğŸ”¥ **RAG-Fusion (Reciprocal Rank Fusion - RRF)**
        """)

        with gr.Row():
            with gr.Column(scale=3):
                chatbot = gr.Chatbot(
                    value=chat_history,
                    type="messages",
                    height=500,
                    label="Sohbet GeÃ§miÅŸi"
                )

                with gr.Row():
                    msg_input = gr.Textbox(
                        placeholder="Sorunuzu buraya yazÄ±n ve Enter'a basÄ±n...",
                        lines=3,
                        label="MesajÄ±nÄ±z",
                        scale=4
                    )
                    submit_btn = gr.Button("GÃ¶nder", variant="primary", scale=1)

                suggestions = gr.Radio(
                    choices=[],
                    label="ğŸ’¡ Ã–nerilen Sorular",
                    visible=False
                )

            with gr.Column(scale=1):
                system_status = gr.HTML(
                    value=f"""
                    <div style="padding: 15px; border-radius: 10px; background: #f0f8ff;">
                        <h3>ğŸ“Š Sistem Durumu</h3>
                        <p><strong>Device:</strong> {device.upper()}</p>
                        <p><strong>BM25 ParÃ§alarÄ±:</strong> {len(rag_system.documents_for_bm25) if rag_system.documents_for_bm25 else 0}</p>
                        <p><strong>Model:</strong> Llama3</p>
                        <p><strong>Cache:</strong> Aktif</p>
                        <p><strong>Retrieval:</strong> Hibrit (RRF)</p>
                        <p><strong>Chunking:</strong> Small-to-Big</p>
                    </div>
                    """
                )

                response_details = gr.JSON(
                    label="ğŸ“‹ Son YanÄ±t DetaylarÄ±",
                    visible=True
                )

                performance_metrics = gr.HTML(
                    value="<div style='padding: 10px;'><h4>âš¡ Performans</h4><p>HazÄ±r...</p></div>"
                )
        
        kb_status = gr.HTML(value="<p>Bilgi bankasÄ± hazÄ±r.</p>")

        with gr.Accordion("ğŸ”§ Sistem AyarlarÄ±", open=False):
            with gr.Row():
                parent_chunk_size_slider = gr.Slider(
                    minimum=128, maximum=2048, value=config.parent_chunk_size,
                    label="Parent Chunk Size", info="Ana belge parÃ§alama boyutu (LLM'e verilen baÄŸlam)"
                )
                parent_chunk_overlap_slider = gr.Slider(
                    minimum=0, maximum=512, value=config.parent_chunk_overlap,
                    label="Parent Chunk Overlap", info="Ana belge parÃ§alarÄ± arasÄ± Ã§akÄ±ÅŸma"
                )
                child_chunk_size_slider = gr.Slider(
                    minimum=16, maximum=512, value=config.child_chunk_size,
                    label="Child Chunk Size", info="Alt belge parÃ§alama boyutu (Embedding ve arama iÃ§in)"
                )
                child_chunk_overlap_slider = gr.Slider(
                    minimum=0, maximum=128, value=config.child_chunk_overlap,
                    label="Child Chunk Overlap", info="Alt belge parÃ§alarÄ± arasÄ± Ã§akÄ±ÅŸma"
                )
            
            with gr.Row():
                top_k_retrieval_slider = gr.Slider(
                    minimum=1, maximum=30, value=config.top_k_retrieval,
                    label="Top-K Retrieval", info="RRF Ã¶ncesi kaÃ§ belge getirilsin"
                )
                top_k_rerank_slider = gr.Slider(
                    minimum=1, maximum=15, value=config.top_k_rerank,
                    label="Top-K Rerank", info="Rerank sonrasÄ± kaÃ§ belge LLM'e gÃ¶nderilsin"
                )
                temperature_slider = gr.Slider(
                    minimum=0.0, maximum=1.0, value=config.temperature,
                    label="Temperature", info="LLM yaratÄ±cÄ±lÄ±k seviyesi"
                )
                embedding_batch_size_slider = gr.Slider(
                    minimum=1, maximum=256, value=config.embedding_batch_size,
                    label="Embedding Batch Size", info="Embedding iÅŸlemi iÃ§in aynÄ± anda iÅŸlenen parÃ§a sayÄ±sÄ±"
                )
                vectorstore_add_batch_size_slider = gr.Slider(
                    minimum=1, maximum=5000, value=config.vectorstore_add_batch_size,
                    label="Vectorstore Add Batch Size", info="VektÃ¶r veritabanÄ±na aynÄ± anda eklenecek belge (child chunk) sayÄ±sÄ±"
                )


            save_settings_btn = gr.Button("âš™ï¸ AyarlarÄ± Kaydet ve Uygula", variant="secondary")

            def save_settings(parent_chunk_size, parent_chunk_overlap, child_chunk_size, child_chunk_overlap, top_k_retrieval, top_k_rerank, temperature, embedding_batch_size, vectorstore_add_batch_size):
                config.parent_chunk_size = int(parent_chunk_size)
                config.parent_chunk_overlap = int(parent_chunk_overlap)
                config.child_chunk_size = int(child_chunk_size)
                config.child_chunk_overlap = int(child_chunk_overlap)
                config.top_k_retrieval = int(top_k_retrieval)
                config.top_k_rerank = int(top_k_rerank)
                config.temperature = float(temperature)
                config.embedding_batch_size = int(embedding_batch_size)
                config.vectorstore_add_batch_size = int(vectorstore_add_batch_size)

                rag_system.embeddings.batch_size = config.embedding_batch_size
                
                logger.info("âš™ï¸ Ayarlar gÃ¼ncellendi. Bilgi bankasÄ± artÄ±mlÄ± olarak gÃ¼ncelleniyor...")
                try:
                    rag_system.build_knowledge_base(force_rebuild=False)
                    rag_system.initialize_llm_chain()
                    return f"<p style='color: green;'>âœ… Ayarlar kaydedildi ve bilgi bankasÄ± gÃ¼ncellendi!</p>"
                except Exception as e:
                    logger.error(f"âŒ AyarlarÄ± uygularken veya bilgi bankasÄ±nÄ± gÃ¼ncellerken hata: {e}")
                    return f"<p style='color: red;'>âŒ Hata: Ayarlar uygulanamadÄ±! {str(e)}</p>"


            save_settings_btn.click(
                fn=save_settings,
                inputs=[
                    parent_chunk_size_slider, parent_chunk_overlap_slider,
                    child_chunk_size_slider, child_chunk_overlap_slider,
                    top_k_retrieval_slider, top_k_rerank_slider,
                    temperature_slider, embedding_batch_size_slider,
                    vectorstore_add_batch_size_slider
                ],
                outputs=kb_status
            )


        with gr.Accordion("ğŸ“š Bilgi BankasÄ± YÃ¶netimi", open=False):
            rebuild_btn = gr.Button("ğŸ”„ Bilgi BankasÄ±nÄ± Yeniden OluÅŸtur (Tamamen)", variant="stop")
            update_btn = gr.Button("â¬†ï¸ DeÄŸiÅŸiklikleri Kontrol Et ve GÃ¼ncelle", variant="primary")

            def rebuild_knowledge_base_full():
                logger.info("Full rebuild initiated.")
                try:
                    rag_system.build_knowledge_base(force_rebuild=True)
                    rag_system.initialize_llm_chain()
                    return f"<p style='color: green;'>âœ… Bilgi bankasÄ± tamamen yeniden oluÅŸturuldu! ({len(rag_system.documents_for_bm25)} child belge parÃ§asÄ±)</p>"
                except Exception as e:
                    logger.error(f"âŒ Full rebuild failed: {e}")
                    return f"<p style='color: red;'>âŒ Hata: {str(e)}</p>"

            def update_knowledge_base_incremental():
                logger.info("Incremental update initiated.")
                try:
                    rag_system.build_knowledge_base(force_rebuild=False)
                    rag_system.initialize_llm_chain()
                    return f"<p style='color: green;'>âœ… Bilgi bankasÄ± artÄ±mlÄ± olarak gÃ¼ncellendi! ({len(rag_system.documents_for_bm25)} gÃ¼ncel child belge parÃ§asÄ±)</p>"
                except Exception as e:
                    logger.error(f"âŒ Incremental update failed: {e}")
                    return f"<p style='color: red;'>âŒ Hata: {str(e)}</p>"


            rebuild_btn.click(
                fn=rebuild_knowledge_base_full,
                outputs=kb_status
            )
            update_btn.click(
                fn=update_knowledge_base_incremental,
                outputs=kb_status
            )


    return demo

def main():
    """Ana fonksiyon"""
    try:
        logger.info("ğŸš€ Starting Advanced RAG System...")

        demo = create_gradio_interface()

        if demo is None:
            logger.error("âŒ Failed to create Gradio interface!")
            return

        logger.info("ğŸŒ Starting Gradio server...")
        demo.launch(
            server_name="0.0.0.0",
            server_port=7860,
            share=False,
            debug=False,
            show_error=True,
            inbrowser=True
        )

    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Sistem kullanÄ±cÄ± tarafÄ±ndan durduruldu")
    except Exception as e:
        logger.error(f"âŒ Sistem hatasÄ±: {e}")
        raise

if __name__ == "__main__":
    main()
